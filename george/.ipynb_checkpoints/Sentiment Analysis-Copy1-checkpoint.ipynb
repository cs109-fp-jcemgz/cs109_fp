{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Sentiment Analysis on FOMC Statements](#Sentiment-Analysis)\n",
    "\t* [Part 1: Preparations and Topic Analysis](#Q1-Predicting-Elections-from-Samples)\n",
    "\t\t* [1.1 Natural Language Processing](#Natural-Language-Processing)\n",
    "\t\t* [1.2 Topic Extraction](#Topic-Extraction)\n",
    "    * [Part 2: Naive Bayes Classification](#Part-2:-Naive-Bayes-Approach)\n",
    "        * [2.1 First Approach: Training a Naive Bayes Classifier](#2.1:-First-Approach:-Training-a-Naive-Bayes-Classifier)\n",
    "        * [2.2 Finding the most important features](#2.2:-Most-important-features)\n",
    "    * [Part 3: Second Approach: Using the Loughran-McDonald Financial Dictionary](#Part-3:-Second-Approach:-Using-the-Loughran-McDonald-Financial-Dictionary)\n",
    "        * [3.1: Adjusting the dictionary for our needs](#3.1:-Adjusting-the-dictionary-for-our-needs)\n",
    "        * [3.2: Most positive and most negative sentences](#3.2:-Most-positive-and-most-negative-sentences)\n",
    "        * [3.3: Classification using new probabilities](#3.3:-Classification-using-new-probabilities)\n",
    "    * [Part 4: \"Directional Policy\": Lengths of Statements and Federal Reserve Policy](#Part-4:-\"Directional-Policy\":-Lengths-of-Statements-and-Federal-Reserve-Policy)\n",
    "        * [4.1: Regressing outcome against length](#4.1:-Regressing-outcome-against-length)\n",
    "        * [4.2: Commentary](#4.2: Commentary)\n",
    "    * [Part 5: Ensemble method](#Part-5:-Ensemble-method)\n",
    "    * [Part 6: Conclusion](#Part-6:-Conclusion)\n",
    "        * [6.1: Results](#6.1:-Results)\n",
    "        * [6.2: Evaluation](#6.2:-Evaluation)\n",
    "\t\t\t\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pyquery import PyQuery as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preparations and Topic Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "We begin by parsing the text and pre-processing it to prepare it for Latent Dirichlet Analysis. This step is meant to remove stopwords and identify nouns and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FOMC statements are full of phrases like \"growth is expected to continue--given the current data--at a moderate pace\". The two hyphens should be treated as a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function to find the nouns and adjectives of the text. The function returns a tuple where the first element is a list of lists, where each list includes the nouns from a sentence. The second element is a list of lists, where each list includes the adjectives from a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modified_get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ', ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        \n",
    "        # Skip the first three sentences that include the HTML\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "#         if i in range(1,4):\n",
    "#             continue\n",
    "            \n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "            \n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2[1:], descriptives2[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the fomc_mins_all dictionary that we created in the Scraping.ipynb iPython Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"fomc_mins_all.json\", \"rb\") as infile:\n",
    "    fomc_mins = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check how our modified_get_parts function would deal with a sample FOMC statement, printing out the first two noun lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'buildmenu',\n",
       "  u'policy',\n",
       "  u'minute',\n",
       "  u'meeting',\n",
       "  u'office',\n",
       "  u'governor',\n",
       "  u'present',\n",
       "  u'member',\n",
       "  u'president',\n",
       "  u'economist',\n",
       "  u'dev'],\n",
       " [u'governor',\n",
       "  u'governor',\n",
       "  u'adviser',\n",
       "  u'governor',\n",
       "  u'governor',\n",
       "  u'governor',\n",
       "  u'merten',\n",
       "  u'interval',\n",
       "  u'meeting',\n",
       "  u'subcommittee',\n",
       "  u'communication',\n",
       "  u'issue']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_get_parts(pq(fomc_mins['20140730']).text())[0][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run modified_get_parts on each fomc statement and create a new dictionary fomc_parts. We strip each statement of the html at its start before passing it to modified_get_parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fomc_parts = {}\n",
    "for key in fomc_mins.keys():\n",
    "    fomc_parts[key] = modified_get_parts(pq(fomc_mins[key]).text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fomc_parts_file = open(\"fomc_parts.json\", \"wb\")\n",
    "json.dump(fomc_parts, fomc_parts_file)\n",
    "fomc_parts_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create two lists, a list of nouns for each sentence, and a flattened list of all the nouns which we will create to produce a dictionary (needed as an argument for the LDA function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][0]\n",
    "    for nounlist in nouns:\n",
    "        nvocab.append(nounlist)\n",
    "\n",
    "flattenednvocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][0]\n",
    "    for nounlist in nouns:\n",
    "        for n in nounlist:\n",
    "            flattenednvocab.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "frequency = Counter(flattenednvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word = {}; vocab = {}\n",
    "for i,word in enumerate(frequency.keys()):\n",
    "    vocab[word] = i \n",
    "    id2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def sentencelist(sentence):\n",
    "    d = defaultdict(int); group = []\n",
    "    for word in sentence:\n",
    "        word_id = vocab[word] \n",
    "        d[word_id] += 1 \n",
    "    group = [(a, d[a]) for a in d.keys()]\n",
    "    return group\n",
    "corpus = [sentencelist(sentence) for sentence in nvocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda2 = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = id2word, update_every=1, chunksize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.034*price + 0.026*quarter + 0.022*month + 0.021*consumer + 0.020*business + 0.020*spending + 0.017*security + 0.016*increase + 0.014*energy + 0.014*sale',\n",
       " u'0.042*inflation + 0.036*rate + 0.033*market + 0.032*policy + 0.031*growth + 0.023*participant + 0.021*member + 0.019*condition + 0.018*price + 0.017*labor']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some commment on the topic extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2559, 1)]\n",
      "[(0, 0.25000565410201342), (1, 0.74999434589798664)]\n",
      "member\n",
      "==========================================\n",
      "[(1600, 1), (2361, 1), (2470, 1), (641, 1)]\n",
      "[(0, 0.58921608777725187), (1, 0.41078391222274807)]\n",
      "issue bid ability dealer\n",
      "==========================================\n",
      "[(1494, 1), (134, 1), (2582, 1)]\n",
      "[(0, 0.87257596967673179), (1, 0.12742403032326824)]\n",
      "paragraph currency transaction\n",
      "==========================================\n",
      "[(1040, 1), (2137, 1), (834, 1), (163, 1), (140, 1)]\n",
      "[(0, 0.90541852844501414), (1, 0.094581471554985752)]\n",
      "sale exchange spot purchase end\n",
      "==========================================\n",
      "[(523, 1), (13, 1), (1590, 1), (2591, 1)]\n",
      "[(0, 0.89993003352926704), (1, 0.10006996647073292)]\n",
      "character swap operation drawing\n",
      "==========================================\n",
      "[(68, 1), (388, 1), (105, 1), (1773, 1), (1714, 1), (19, 1), (1398, 1), (676, 2), (2559, 1)]\n",
      "[(0, 0.047940037486703779), (1, 0.95205996251329628)]\n",
      "balance approach assessment announcement wording risk condition time member\n",
      "==========================================\n",
      "[(2624, 1), (1227, 1), (905, 1), (779, 1), (844, 1), (892, 1), (2258, 1), (915, 1), (2649, 1), (924, 1)]\n",
      "[(0, 0.39018662439118418), (1, 0.60981337560881577)]\n",
      "loss improvement quarter market recovery half labor year employment increase\n",
      "==========================================\n",
      "[(2050, 1)]\n",
      "[(0, 0.65147707508491681), (1, 0.34852292491508313)]\n",
      "income\n",
      "==========================================\n",
      "[(1040, 1), (617, 1), (924, 1), (1477, 1), (1061, 1)]\n",
      "[(0, 0.91480923381253454), (1, 0.085190766187465461)]\n",
      "sale lows increase book-value ratio\n",
      "==========================================\n",
      "[(878, 1), (944, 1), (1137, 1), (1242, 1), (1083, 1), (2362, 1)]\n",
      "[(0, 0.07493418985630064), (1, 0.92506581014369937)]\n",
      "policy accommodation period inflation resource slack\n",
      "==========================================\n",
      "[(2050, 1), (2507, 1), (132, 1), (1254, 1), (2376, 1), (2570, 1), (779, 1), (922, 1), (1999, 2), (1424, 1), (1521, 1), (2258, 1), (2643, 1), (1672, 1), (1398, 2), (634, 1), (2555, 1), (2122, 2)]\n",
      "[(0, 0.49049186064341649), (1, 0.50950813935658357)]\n",
      "income addition household output acceleration effect market profit spending investment response labor growth financing condition strengthening productivity business\n",
      "==========================================\n",
      "[(1632, 1), (2177, 1), (740, 1), (1289, 1), (2122, 2), (1999, 1), (1872, 1), (1944, 1), (478, 1), (888, 1), (824, 1), (1946, 1), (1021, 1), (2686, 1), (2559, 1)]\n",
      "[(0, 0.49260828914458527), (1, 0.50739171085541468)]\n",
      "number confidence sign industry business spending development indication inventory nation discussion softness equipment software member\n",
      "==========================================\n",
      "[(2177, 1), (130, 1), (1032, 1), (2570, 1), (781, 1), (878, 1), (1999, 1), (2192, 1), (1297, 1), (335, 1)]\n",
      "[(0, 0.52990586640992088), (1, 0.47009413359007901)]\n",
      "confidence estate price effect wealth policy spending equity outlook consumer\n",
      "==========================================\n",
      "[(2449, 1), (2002, 1), (1004, 1)]\n",
      "[(0, 0.12505329621196135), (1, 0.87494670378803863)]\n",
      "view disinflation prospect\n",
      "==========================================\n",
      "[(1632, 1), (2721, 1), (1604, 1), (19, 1), (999, 1), (779, 2), (878, 1), (944, 1), (1714, 1), (1075, 1), (117, 1), (763, 1), (2559, 1)]\n",
      "[(0, 0.034534257844879082), (1, 0.96546574215512093)]\n",
      "number valuation circumstance risk future market policy accommodation wording expectation room change member\n",
      "==========================================\n",
      "[(965, 1), (2582, 1)]\n",
      "[(0, 0.82846096618901865), (1, 0.17153903381098137)]\n",
      "vote transaction\n",
      "==========================================\n",
      "[(384, 1), (1418, 1), (1137, 1), (2643, 1), (22, 1), (570, 1), (923, 1), (188, 1)]\n",
      "[(0, 0.77212664164798117), (1, 0.22787335835201883)]\n",
      "capacity rate period growth rise manufacturing factory production\n",
      "==========================================\n",
      "[(264, 1), (1564, 1), (1540, 1), (1269, 1), (1038, 1)]\n",
      "[(0, 0.91425886556206615), (1, 0.085741134437933839)]\n",
      "service good trade deficit average\n",
      "==========================================\n",
      "[(1829, 1), (1418, 1), (779, 1), (1137, 1), (1590, 1), (2746, 1)]\n",
      "[(0, 0.26641900574691374), (1, 0.7335809942530862)]\n",
      "fund rate market period operation percent\n",
      "==========================================\n",
      "[(724, 1), (1032, 1), (2570, 1), (1021, 1), (2703, 1), (781, 1), (2173, 1), (1999, 1), (2192, 1), (2226, 1), (2643, 2), (2122, 1), (2349, 1), (89, 1), (335, 1), (924, 1), (1834, 1)]\n",
      "[(0, 0.78492603823577944), (1, 0.21507396176422064)]\n",
      "item price effect equipment housing wealth buildup spending equity stock growth business durable unit consumer increase demand\n",
      "==========================================\n",
      "[(1632, 1), (740, 1), (1834, 2), (1227, 1), (400, 1), (311, 1), (570, 1), (411, 1), (2559, 1)]\n",
      "[(0, 0.42054351641694265), (1, 0.57945648358305724)]\n",
      "number sign demand improvement extent strength manufacturing country member\n",
      "==========================================\n",
      "[(1312, 1), (1382, 1), (968, 1), (1747, 1), (2709, 1), (1657, 1), (2559, 1)]\n",
      "[(0, 0.27478936265215537), (1, 0.72521063734784463)]\n",
      "area apartment city activity building oversupply member\n",
      "==========================================\n",
      "[(1665, 1), (1797, 1), (2248, 1), (924, 1), (2570, 1), (2507, 1), (1583, 1), (1032, 1), (1733, 1), (1075, 1), (1302, 1), (1242, 1), (2364, 1), (1374, 1), (127, 1)]\n",
      "[(0, 0.45344405877980093), (1, 0.54655594122019902)]\n",
      "appreciation supply reversal increase effect addition energy price factor expectation dissipation inflation compensation dollar example\n",
      "==========================================\n",
      "[(2083, 1), (1137, 1), (2123, 1), (1260, 1), (878, 1), (2513, 1), (2559, 1), (959, 1)]\n",
      "[(0, 0.065249820744938319), (1, 0.93475017925506165)]\n",
      "adjustment period directive respect policy proposal member symmetry\n",
      "==========================================\n",
      "[(224, 1), (1665, 1), (2530, 1), (134, 2), (872, 1), (779, 1), (1933, 1), (1137, 1), (2291, 1), (2550, 1), (2137, 1), (1723, 1), (1374, 1)]\n",
      "[(0, 0.96055348002364871), (1, 0.039446519976351326)]\n",
      "trading appreciation value currency group market bit period subset partner exchange relation dollar\n",
      "==========================================\n",
      "[(965, 1), (2582, 1)]\n",
      "[(0, 0.82846096751252285), (1, 0.17153903248747704)]\n",
      "vote transaction\n",
      "==========================================\n",
      "[(721, 1), (1137, 1), (2646, 1), (478, 1), (89, 1)]\n",
      "[(0, 0.69836821768230362), (1, 0.30163178231769633)]\n",
      "level period home inventory unit\n",
      "==========================================\n",
      "[(480, 1), (1032, 1), (1242, 1), (1842, 1), (1583, 1)]\n",
      "[(0, 0.67411904723339922), (1, 0.32588095276660073)]\n",
      "rebound price inflation pace energy\n",
      "==========================================\n",
      "[(224, 1), (2530, 1), (134, 1), (872, 1), (779, 1), (1137, 1), (2550, 1), (2137, 1), (1723, 1), (1374, 1)]\n",
      "[(0, 0.94315994258840652), (1, 0.056840057411593564)]\n",
      "trading value currency group market period partner exchange relation dollar\n",
      "==========================================\n",
      "[(1585, 1), (1834, 1), (2644, 1), (830, 1), (487, 1)]\n",
      "[(0, 0.54950085264441995), (1, 0.45049914735558)]\n",
      "slowdown demand export economy hand\n",
      "==========================================\n",
      "[(163, 1), (840, 1), (1999, 1), (948, 1), (22, 1), (763, 1), (1852, 1)]\n",
      "[(0, 0.91489497300515588), (1, 0.085105026994844221)]\n",
      "purchase date spending advance rise change century\n",
      "==========================================\n",
      "[(1032, 1), (1242, 1), (939, 1), (924, 1), (2559, 1)]\n",
      "[(0, 0.13812913509756417), (1, 0.86187086490243592)]\n",
      "price inflation possibility increase member\n",
      "==========================================\n",
      "[(1025, 1), (1506, 1), (2243, 1), (1137, 1), (1325, 1), (413, 1), (1872, 1), (1297, 1), (1843, 1), (1242, 1), (637, 1), (830, 2), (2559, 1)]\n",
      "[(0, 0.034080900390247755), (1, 0.96591909960975231)]\n",
      "performance information light period regard uncertainty development outlook behavior inflation evidence economy member\n",
      "==========================================\n",
      "[(721, 1), (2716, 1)]\n",
      "[(0, 0.80990028684203397), (1, 0.190099713157966)]\n",
      "level construction\n",
      "==========================================\n",
      "[(1024, 1), (865, 1), (1829, 1), (999, 1), (1032, 1), (1418, 1), (779, 1), (1038, 1), (338, 1), (2643, 1), (1398, 1), (2746, 1)]\n",
      "[(0, 0.043903214744074243), (1, 0.95609678525592579)]\n",
      "objective stability fund future price rate market average reserve growth condition percent\n",
      "==========================================\n",
      "[(1241, 1), (898, 1), (1398, 1), (1590, 2), (999, 1)]\n",
      "[(0, 0.59838216243618103), (1, 0.40161783756381902)]\n",
      "test term condition operation future\n",
      "==========================================\n",
      "[(1499, 1), (676, 1), (1607, 1), (1418, 1), (2746, 1), (347, 1), (446, 1)]\n",
      "[(0, 0.33336531560419497), (1, 0.66663468439580509)]\n",
      "share time reason rate percent unemployment worker\n",
      "==========================================\n",
      "[(1564, 1), (1778, 1), (51, 1), (2132, 1), (924, 1), (1599, 1)]\n",
      "[(0, 0.92727253826853839), (1, 0.072727461731461565)]\n",
      "good aircraft net capital increase shipment\n",
      "==========================================\n",
      "[(480, 1), (1729, 1), (2181, 1), (2214, 1), (905, 1), (1312, 1), (1521, 1), (2643, 1), (924, 1)]\n",
      "[(0, 0.6801192314607909), (1, 0.31988076853920916)]\n",
      "rebound indicator consumption tax quarter area response growth increase\n",
      "==========================================\n",
      "[(1632, 1), (224, 1), (2276, 1), (1733, 1), (956, 1), (842, 1), (779, 1), (1040, 1), (369, 1), (2706, 1), (1813, 1), (1435, 1), (1180, 1), (1418, 1)]\n",
      "[(0, 0.20625610718406853), (1, 0.7937438928159315)]\n",
      "number trading investor factor swing data market sale release trigger strategy movement participant rate\n",
      "==========================================\n",
      "[(2276, 1), (1798, 1), (102, 1), (779, 1), (2573, 1), (2393, 1)]\n",
      "[(0, 0.54335248588758167), (1, 0.45664751411241827)]\n",
      "investor loan asset market issuance class\n",
      "==========================================\n",
      "[(167, 1), (1385, 1), (779, 1), (525, 1), (1294, 2), (1104, 1), (411, 1)]\n",
      "[(0, 0.63131571260648822), (1, 0.36868428739351183)]\n",
      "greek program market spread bond concern country\n",
      "==========================================\n",
      "[(19, 1), (1104, 1), (258, 1), (105, 1), (878, 2), (1872, 1), (1297, 1), (2643, 1), (1108, 1), (2420, 1), (1242, 1), (2107, 1), (830, 1)]\n",
      "[(0, 0.033386173349847927), (1, 0.96661382665015205)]\n",
      "risk concern downside assessment policy development outlook growth forecast staff inflation shock economy\n",
      "==========================================\n",
      "[(1764, 1), (1180, 1), (2643, 1), (1747, 1), (1977, 1), (2716, 1)]\n",
      "[(0, 0.35414125742744557), (1, 0.64585874257255438)]\n",
      "project participant growth activity pipeline construction\n",
      "==========================================\n",
      "[(898, 1), (1190, 1), (1032, 2), (847, 1), (1111, 1), (1242, 1), (1807, 1), (1180, 1)]\n",
      "[(0, 0.69134405657490172), (1, 0.30865594342509828)]\n",
      "term oil price commodity import inflation decline participant\n",
      "==========================================\n",
      "[(163, 1), (676, 1), (1829, 1), (102, 1), (1385, 1), (1418, 1), (140, 1), (1652, 1), (950, 1), (283, 1), (1180, 1), (1981, 1)]\n",
      "[(0, 0.041993766780232082), (1, 0.95800623321976797)]\n",
      "purchase time fund asset program rate end statement language target participant range\n",
      "==========================================\n",
      "[(374, 1), (487, 1), (490, 1), (1872, 1), (2643, 1), (1398, 1)]\n",
      "[(0, 0.073538730349127251), (1, 0.92646126965087272)]\n",
      "reference hand opportunity development growth condition\n",
      "==========================================\n",
      "[(779, 1), (1398, 1), (1590, 1)]\n",
      "[(0, 0.41001825747849657), (1, 0.58998174252150348)]\n",
      "market condition operation\n",
      "==========================================\n",
      "[(1793, 1), (2469, 1), (865, 1), (1032, 1), (311, 1), (2649, 1), (830, 1)]\n",
      "[(0, 0.079469758300326043), (1, 0.9205302416996739)]\n",
      "context progress stability price strength employment economy\n",
      "==========================================\n",
      "[(1600, 1), (1387, 1), (813, 1), (878, 1), (2420, 1), (2260, 1), (1180, 1), (1981, 1)]\n",
      "[(0, 0.05779667403460146), (1, 0.94220332596539846)]\n",
      "issue presentation topic policy staff communication participant range\n",
      "==========================================\n",
      "[(2245, 1), (551, 1), (1002, 1), (1517, 1), (176, 1), (1137, 1), (1820, 1), (1757, 1), (2559, 2)]\n",
      "[(0, 0.16787831179977952), (1, 0.83212168820022048)]\n",
      "agenda election oath advice office period meeting individual member\n",
      "==========================================\n",
      "[(965, 1), (1063, 1)]\n",
      "[(0, 0.83294458799389448), (1, 0.16705541200610549)]\n",
      "vote title\n",
      "==========================================\n",
      "[(544, 2), (2277, 1), (941, 1), (1102, 1), (207, 1), (54, 1), (2618, 1)]\n",
      "[(0, 0.94427462286135644), (1, 0.055725377138643659)]\n",
      "security day repurchase calendar bank agreement government\n",
      "==========================================\n",
      "[(1675, 1), (1260, 1), (965, 1), (1063, 1)]\n",
      "[(0, 0.89143581695494079), (1, 0.1085641830450592)]\n",
      "instruction respect vote title\n",
      "==========================================\n",
      "[(1459, 1), (1820, 1), (965, 1), (718, 1)]\n",
      "[(0, 0.46079821009050426), (1, 0.53920178990949585)]\n",
      "action meeting vote minute\n",
      "==========================================\n",
      "[(721, 1), (2386, 1), (915, 1), (2200, 1), (570, 1), (1375, 1), (188, 1), (446, 1), (1087, 2)]\n",
      "[(0, 0.95093219036950893), (1, 0.049067809630491066)]\n",
      "level post-world year high manufacturing month production worker workweek\n",
      "==========================================\n",
      "[(898, 1), (905, 1), (77, 1), (2063, 1), (1424, 1), (2643, 1)]\n",
      "[(0, 0.8063499095908232), (1, 0.19365009040917686)]\n",
      "term quarter permit structure investment growth\n",
      "==========================================\n",
      "[(1032, 2), (2121, 1), (1583, 1), (721, 1), (335, 1), (924, 1)]\n",
      "[(0, 0.93015433910075318), (1, 0.069845660899246834)]\n",
      "price index energy level consumer increase\n",
      "==========================================\n",
      "[(134, 1), (685, 1), (531, 1), (1747, 1), (1144, 1), (1374, 1)]\n",
      "[(0, 0.64235917364080464), (1, 0.35764082635919542)]\n",
      "currency datum background activity mark dollar\n",
      "==========================================\n",
      "[(866, 1), (1834, 1), (1999, 1), (721, 1), (1842, 1), (1723, 1), (924, 1)]\n",
      "[(0, 0.73781283304923562), (1, 0.26218716695076444)]\n",
      "course demand spending level pace relation increase\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "for bow in corpus[0:900:15]:\n",
    "    print bow\n",
    "    print lda2.get_document_topics(bow)\n",
    "    print \" \".join([id2word[e[0]] for e in bow])\n",
    "    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment on carrying out naive bayes and the fact that we will do it 2 ways.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: First Approach: Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the vocabulary of adjectives that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flattened_adj_vocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][1]\n",
    "    for nounlist in nouns:\n",
    "        for n in nounlist:\n",
    "            flattened_adj_vocab.append(n)\n",
    "\n",
    "frequency2 = Counter(flattened_adj_vocab)\n",
    "id2adj = {}; adjvocab = {}\n",
    "for i,word in enumerate(frequency2.keys()):\n",
    "    adjvocab[word] = i \n",
    "    id2adj[i] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the adjectives in each statement into a single list and thereby have a list of adjectives for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][1]\n",
    "    for nounlist in nouns:\n",
    "        X.append(nounlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Response Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"actions_05-15.json\", \"rb\") as infile:\n",
    "    decisions = json.load(infile)\n",
    "len(decisions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions_work = decisions.copy()\n",
    "new_decisions = {}\n",
    "for key in fomc_mins:\n",
    "    if key in decisions_work.keys():\n",
    "        new_decisions[key] = decisions_work[key][-1]\n",
    "        del decisions_work[key]\n",
    "        \n",
    "len(new_decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a group of statements and decisions, where the statements are releases made at certain dates and decisions are interest rate decisions made at certain dates. We now group the statements between each decision into one string. For example, if a decision is made on 09/28/2014 and the next one is made on 10/18/2014, then the statements in between would be grouped together. This allows us to match the dimensions of our input and our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in fomc_mins.keys():\n",
    "    if key < min(decisions.keys()):\n",
    "        del fomc_mins[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fomc_mins)\n",
    "for key in fomc_mins.keys():\n",
    "    if key not in decisions.keys():\n",
    "        decisions[key] = [0,0,0,0,0]\n",
    "len(fomc_mins.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'alternate',\n",
       " u'special',\n",
       " u'alternate',\n",
       " u'elected',\n",
       " u'alternate',\n",
       " u'alternate',\n",
       " u'alternate',\n",
       " u'alternate',\n",
       " u'alternate',\n",
       " u'alternate',\n",
       " u'effective',\n",
       " u'alternate',\n",
       " u'alternate',\n",
       " u'effective',\n",
       " u'unanimous',\n",
       " u'official',\n",
       " u'official',\n",
       " u'unanimous',\n",
       " u'unanimous',\n",
       " u'subject',\n",
       " u'satisfactory',\n",
       " u'unanimous',\n",
       " u'small',\n",
       " u'electronic',\n",
       " u'confidential',\n",
       " u'unanimous',\n",
       " u'open',\n",
       " u'necessary',\n",
       " u'recent',\n",
       " u'domestic',\n",
       " u'direct',\n",
       " u'open',\n",
       " u'foreign',\n",
       " u'international',\n",
       " u'regular',\n",
       " u'individual',\n",
       " u'aggregate',\n",
       " u'forward',\n",
       " u'domestic',\n",
       " u'direct',\n",
       " u'competitive',\n",
       " u'reasonable',\n",
       " u'individual',\n",
       " u'pursuant',\n",
       " u'direct',\n",
       " u'competitive',\n",
       " u'reasonable',\n",
       " u'individual',\n",
       " u'effective',\n",
       " u'open',\n",
       " u'overnight',\n",
       " u'competitive',\n",
       " u'lending',\n",
       " u'consistent',\n",
       " u'reasonable',\n",
       " u'total',\n",
       " u'specific',\n",
       " u'single',\n",
       " u'effective',\n",
       " u'open',\n",
       " u'short-term',\n",
       " u'foreign',\n",
       " u'international',\n",
       " u'fiscal',\n",
       " u'pursuant',\n",
       " u'comparable',\n",
       " u'available',\n",
       " u'appropriate',\n",
       " u'corresponding',\n",
       " u'foreign',\n",
       " u'international',\n",
       " u'fiscal',\n",
       " u'appropriate',\n",
       " u'intermeeting',\n",
       " u'exceptional',\n",
       " u'intended',\n",
       " u'federal',\n",
       " u'recent',\n",
       " u'long-run',\n",
       " u'sustainable',\n",
       " u'economic',\n",
       " u'economic',\n",
       " u'financial',\n",
       " u'monetary',\n",
       " u'intermeeting',\n",
       " u'consistent',\n",
       " u'feasible',\n",
       " u'domestic',\n",
       " u'open',\n",
       " u'pursuant',\n",
       " u'fiscal',\n",
       " u'official',\n",
       " u'foreign',\n",
       " u'international',\n",
       " u'eligible',\n",
       " u'short-term',\n",
       " u'so-called',\n",
       " u'repo',\n",
       " u'unanimous',\n",
       " u'foreign',\n",
       " u'necessary',\n",
       " u'foreign',\n",
       " u'express',\n",
       " u'pursuant',\n",
       " u'procedural',\n",
       " u'following',\n",
       " u'foreign',\n",
       " u'forward',\n",
       " u'open',\n",
       " u'foreign',\n",
       " u'monetary',\n",
       " u'international',\n",
       " u'financial',\n",
       " u'canadian',\n",
       " u'danish',\n",
       " u'sterling',\n",
       " u'japanese',\n",
       " u'mexican',\n",
       " u'norwegian',\n",
       " u'swedish',\n",
       " u'swiss',\n",
       " u'outstanding',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'reciprocal',\n",
       " u'outstanding',\n",
       " u'exceptional',\n",
       " u'overall',\n",
       " u'open',\n",
       " u'foreign',\n",
       " u'overall',\n",
       " u'open',\n",
       " u'foreign',\n",
       " u'defined',\n",
       " u'net',\n",
       " u'individual',\n",
       " u'single',\n",
       " u'foreign',\n",
       " u'defined',\n",
       " u'outstanding',\n",
       " u'future',\n",
       " u'outstanding',\n",
       " u'future',\n",
       " u'reciprocal',\n",
       " u'following',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'equivalent',\n",
       " u'proposed',\n",
       " u'new',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'central',\n",
       " u'normal',\n",
       " u'foreign',\n",
       " u'central',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'central',\n",
       " u'foreign',\n",
       " u'specific',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'adequate',\n",
       " u'average',\n",
       " u'calculated',\n",
       " u'appropriate',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'central',\n",
       " u'pursuant',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'alternate',\n",
       " u'manager',\n",
       " u'recent',\n",
       " u'contemplated',\n",
       " u'manager',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'appropriate',\n",
       " u'pertinent',\n",
       " u'foreign',\n",
       " u'appropriate',\n",
       " u'foreign',\n",
       " u'unanimous',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'disorderly',\n",
       " u'consistent',\n",
       " u'forward',\n",
       " u'foreign',\n",
       " u'reciprocal',\n",
       " u'selected',\n",
       " u'foreign',\n",
       " u'central',\n",
       " u'central',\n",
       " u'international',\n",
       " u'monetary',\n",
       " u'adjust',\n",
       " u'probable',\n",
       " u'particular',\n",
       " u'facilitate',\n",
       " u'foreign',\n",
       " u'continuous',\n",
       " u'appropriate',\n",
       " u'foreign',\n",
       " u'monetary',\n",
       " u'consistent',\n",
       " u'unanimous',\n",
       " u'procedural',\n",
       " u'foreign',\n",
       " u'procedural',\n",
       " u'pursuant',\n",
       " u'foreign',\n",
       " u'foreign',\n",
       " u'manager',\n",
       " u'following',\n",
       " u'procedural',\n",
       " u'foreign',\n",
       " u'pursuant',\n",
       " u'feasible',\n",
       " u'available',\n",
       " u'overall',\n",
       " u'open',\n",
       " u'foreign',\n",
       " u'recent',\n",
       " u'regular',\n",
       " u'single',\n",
       " u'foreign',\n",
       " u'substantial',\n",
       " u'particular',\n",
       " u'foreign',\n",
       " u'larger',\n",
       " u'feasible',\n",
       " u'available',\n",
       " u'feasible',\n",
       " u'available',\n",
       " u'overall',\n",
       " u'open',\n",
       " u'foreign',\n",
       " u'recent',\n",
       " u'regular',\n",
       " u'foreign',\n",
       " u'larger',\n",
       " u'routine',\n",
       " u'mortgage-backed',\n",
       " u'federal',\n",
       " u'potential',\n",
       " u'direct',\n",
       " u'large',\n",
       " u'federal',\n",
       " u'reduced',\n",
       " u'large',\n",
       " u'federal',\n",
       " u'sizable',\n",
       " u'preferred',\n",
       " u'permanent',\n",
       " u'outright',\n",
       " u'mortgage-backed',\n",
       " u'feasible',\n",
       " u'sizeable',\n",
       " u'open',\n",
       " u'outright',\n",
       " u'mortgage-backed',\n",
       " u'eligible',\n",
       " u'unanimous',\n",
       " u'recent',\n",
       " u'foreign',\n",
       " u'open',\n",
       " u'foreign',\n",
       " u'previous',\n",
       " u'domestic',\n",
       " u'financial',\n",
       " u'open',\n",
       " u'federal',\n",
       " u'unanimous',\n",
       " u'working',\n",
       " u'public',\n",
       " u'related',\n",
       " u'longer-term',\n",
       " u'limited',\n",
       " u'particular',\n",
       " u'economic',\n",
       " u'flexible',\n",
       " u'economic',\n",
       " u'potential',\n",
       " u'possible',\n",
       " u'complete',\n",
       " u'current',\n",
       " u'expressed',\n",
       " u'accelerated',\n",
       " u'potential',\n",
       " u'sufficient',\n",
       " u'large',\n",
       " u'possible',\n",
       " u'individual',\n",
       " u'semi-annual',\n",
       " u'general',\n",
       " u'little',\n",
       " u'current',\n",
       " u'significant',\n",
       " u'minor',\n",
       " u'possible',\n",
       " u'minor',\n",
       " u'current',\n",
       " u'economic',\n",
       " u'monetary',\n",
       " u'intermeeting',\n",
       " u'robust',\n",
       " u'fourth',\n",
       " u'exceptional',\n",
       " u'solid',\n",
       " u'final',\n",
       " u'residential',\n",
       " u'high',\n",
       " u'likely',\n",
       " u'posted',\n",
       " u'moderate',\n",
       " u'fourth',\n",
       " u'strong',\n",
       " u'economic',\n",
       " u'private',\n",
       " u'small',\n",
       " u'fourth',\n",
       " u'closing',\n",
       " u'private',\n",
       " u'nonfarm',\n",
       " u'average',\n",
       " u'monthly',\n",
       " u'fourth',\n",
       " u'indicative',\n",
       " u'weak',\n",
       " u'clear',\n",
       " u'average',\n",
       " u'monthly',\n",
       " u'previous',\n",
       " u'holiday-related',\n",
       " u'retail',\n",
       " u'average',\n",
       " u'professional',\n",
       " u'average',\n",
       " u'weekly',\n",
       " u'nonsupervisory',\n",
       " u'previous',\n",
       " u'aggregate',\n",
       " u'nonfarm',\n",
       " u'fourth',\n",
       " u'quarterly',\n",
       " u'weak',\n",
       " u'initial',\n",
       " u'improved',\n",
       " u'industrial',\n",
       " u'fourth',\n",
       " u'total',\n",
       " u'industrial',\n",
       " u'fastest',\n",
       " u'second',\n",
       " u'solid',\n",
       " u'widespread',\n",
       " u'high-tech',\n",
       " u'total',\n",
       " u'industrial',\n",
       " u'fourth',\n",
       " u'fourth',\n",
       " u'overall',\n",
       " u'solid',\n",
       " u'unchanged',\n",
       " u'long-term',\n",
       " u'light',\n",
       " u'fourth-quarter',\n",
       " u'torrid',\n",
       " u'retail',\n",
       " u'consistent',\n",
       " u'solid',\n",
       " u'real',\n",
       " u'personal',\n",
       " u'fourth',\n",
       " u'real',\n",
       " u'disposable',\n",
       " u'personal',\n",
       " u'recent',\n",
       " u'greater',\n",
       " u'average',\n",
       " u'robust',\n",
       " u'exceptional',\n",
       " u'highest',\n",
       " u'consecutive',\n",
       " u'record',\n",
       " u'higher',\n",
       " u'average',\n",
       " u'monthly',\n",
       " u'available',\n",
       " u'fourth',\n",
       " u'moderate',\n",
       " u'fourth',\n",
       " u'light',\n",
       " u'high-tech',\n",
       " u'real',\n",
       " u'sizable',\n",
       " u'fourth',\n",
       " u'little',\n",
       " u'double-digit',\n",
       " u'outside',\n",
       " u'high',\n",
       " u'nominal',\n",
       " u'unchanged',\n",
       " u'fourth',\n",
       " u'upward',\n",
       " u'consistent',\n",
       " u'steady',\n",
       " u'real',\n",
       " u'private',\n",
       " u'nonresidential',\n",
       " u'fourth',\n",
       " u'commercial',\n",
       " u'health-care',\n",
       " u'average',\n",
       " u'nominal',\n",
       " u'unchanged',\n",
       " u'previous',\n",
       " u'consecutive',\n",
       " u'significant',\n",
       " u'monthly',\n",
       " u'little',\n",
       " u'non-auto',\n",
       " u'brisk',\n",
       " u'strong',\n",
       " u'recent',\n",
       " u'international',\n",
       " u'lowest',\n",
       " u'recent',\n",
       " u'economic',\n",
       " u'major',\n",
       " u'foreign',\n",
       " u'industrial',\n",
       " u'fourth',\n",
       " u'japanese',\n",
       " u'industrial',\n",
       " u'euro-area',\n",
       " u'retail',\n",
       " u'maintained',\n",
       " u'canadian',\n",
       " u'strong',\n",
       " u'fourth',\n",
       " u'significant',\n",
       " u'overall',\n",
       " u'flat',\n",
       " u'past',\n",
       " u'small',\n",
       " u'core',\n",
       " u'large',\n",
       " u'sizable',\n",
       " u'moderate',\n",
       " u'previous',\n",
       " u'finished',\n",
       " u'previous',\n",
       " u'substantial',\n",
       " u'core',\n",
       " u'little',\n",
       " u'average',\n",
       " u'hourly',\n",
       " u'nonsupervisory',\n",
       " u'private',\n",
       " u'nonfarm',\n",
       " u'consistent',\n",
       " u'federal',\n",
       " u'downside',\n",
       " u'sustainable',\n",
       " u'equal',\n",
       " u'unwelcome',\n",
       " u'recent',\n",
       " u'equal',\n",
       " u'low',\n",
       " u'considerable',\n",
       " u'federal',\n",
       " u'financial',\n",
       " u'unchanged',\n",
       " u'second',\n",
       " u'unwelcome',\n",
       " u'recent',\n",
       " u'equal',\n",
       " u'persistent',\n",
       " u'rapid',\n",
       " u'modest',\n",
       " u'expected',\n",
       " u'intermediate',\n",
       " u'longer-term',\n",
       " u'nominal',\n",
       " u'intermeeting',\n",
       " u'inflation-indexed',\n",
       " u'nominal',\n",
       " u'real',\n",
       " u'reduced',\n",
       " u'investment-grade',\n",
       " u'speculative-grade',\n",
       " u'major',\n",
       " u'positive',\n",
       " u'major',\n",
       " u'intermeeting',\n",
       " u'ongoing',\n",
       " u'current',\n",
       " u'primary',\n",
       " u'fourth',\n",
       " u'consecutive',\n",
       " u'monthly',\n",
       " u'fourth',\n",
       " u'largest',\n",
       " u'consistent',\n",
       " u'liquid',\n",
       " u'lesser',\n",
       " u'retail',\n",
       " u'mutual',\n",
       " u'large',\n",
       " u'previous',\n",
       " u'heavy',\n",
       " u'economic',\n",
       " u'second',\n",
       " u'current',\n",
       " u'ongoing',\n",
       " u'visible',\n",
       " u'considerable',\n",
       " u'fiscal',\n",
       " u'monetary',\n",
       " u'aggregate',\n",
       " u'solid',\n",
       " u'strong',\n",
       " u'permanent',\n",
       " u'favorable',\n",
       " u'slight',\n",
       " u'downward',\n",
       " u'core',\n",
       " u'ongoing',\n",
       " u'current',\n",
       " u'prospective',\n",
       " u'economic',\n",
       " u'available',\n",
       " u'earlier',\n",
       " u'robust',\n",
       " u'economic',\n",
       " u'mid',\n",
       " u'likely',\n",
       " u'solid',\n",
       " u'widespread',\n",
       " u'improved',\n",
       " u'significant',\n",
       " u'favorable',\n",
       " u'economic',\n",
       " u'stimulative',\n",
       " u'fiscal',\n",
       " u'monetary',\n",
       " u'accommodative',\n",
       " u'financial',\n",
       " u'positive',\n",
       " u'strong',\n",
       " u'expressed',\n",
       " u'economic',\n",
       " u'significant',\n",
       " u'positive',\n",
       " u'above-trend',\n",
       " u'economic',\n",
       " u'likely',\n",
       " u'wide',\n",
       " u'concurrent',\n",
       " u'modest',\n",
       " u'current',\n",
       " u'subdued',\n",
       " u'little',\n",
       " u'semi-annual',\n",
       " u'monetary',\n",
       " u'individual',\n",
       " u'vigorous',\n",
       " u'economic',\n",
       " u'low',\n",
       " u'real',\n",
       " u'fourth',\n",
       " u'fourth',\n",
       " u'central',\n",
       " u'civilian',\n",
       " u'fourth',\n",
       " u'widespread',\n",
       " u'new',\n",
       " u'improved',\n",
       " u'financial',\n",
       " u'general',\n",
       " u'favorable',\n",
       " u'temporary',\n",
       " u'new',\n",
       " u'anecdotal',\n",
       " u'indicative',\n",
       " u'appreciable',\n",
       " u'cost-saving',\n",
       " u'productive',\n",
       " u'new',\n",
       " u'driving',\n",
       " u'likely',\n",
       " u'economic',\n",
       " u'business',\n",
       " u'low',\n",
       " u'brisk',\n",
       " u'current',\n",
       " u'commensurate',\n",
       " u'fourth',\n",
       " u'confident',\n",
       " u'high',\n",
       " u'new',\n",
       " u'protracted',\n",
       " u'weaker',\n",
       " u'typical',\n",
       " u'similar',\n",
       " u'earlier',\n",
       " u'key',\n",
       " u'new',\n",
       " u'soft',\n",
       " u'modest',\n",
       " u'positive',\n",
       " u'initial',\n",
       " u'fourth',\n",
       " u'faster',\n",
       " u'likely',\n",
       " u'aggregate',\n",
       " u'extraordinary',\n",
       " u'recent',\n",
       " u'subject',\n",
       " u'considerable',\n",
       " u'major',\n",
       " u'stimulative',\n",
       " u'fiscal',\n",
       " u'monetary',\n",
       " u'real',\n",
       " u'increased',\n",
       " u'economic',\n",
       " u'solid',\n",
       " u'overall',\n",
       " u'elevated',\n",
       " u'earlier',\n",
       " u'high',\n",
       " u'recent',\n",
       " u'high',\n",
       " u'robust',\n",
       " u'fiscal',\n",
       " u'considerable',\n",
       " u'large',\n",
       " u'nearer',\n",
       " u'fiscal',\n",
       " u'robust',\n",
       " u'fiscal',\n",
       " u'expressed',\n",
       " u'longer-run',\n",
       " u'large',\n",
       " u'federal',\n",
       " u'future',\n",
       " u'international',\n",
       " u'economic',\n",
       " u'foreign',\n",
       " u'ongoing',\n",
       " u'net',\n",
       " u'small',\n",
       " u'domestic',\n",
       " u'economic',\n",
       " u'external',\n",
       " u'domestic',\n",
       " u'financial',\n",
       " u'core',\n",
       " u'likely',\n",
       " u'ongoing',\n",
       " u'current',\n",
       " u'anticipated',\n",
       " u'downward',\n",
       " u'loose',\n",
       " u'uncertain',\n",
       " u'core',\n",
       " u'likely',\n",
       " u'probable',\n",
       " u'modest',\n",
       " u'likely',\n",
       " u'rapid',\n",
       " u'expected',\n",
       " u'aggregate',\n",
       " u'core',\n",
       " u'considerable',\n",
       " u'current',\n",
       " u'monetary',\n",
       " u'fiscal',\n",
       " u'economic',\n",
       " u'high',\n",
       " u'current',\n",
       " u'slower',\n",
       " u'sizable',\n",
       " u'able',\n",
       " u'higher',\n",
       " u'overall',\n",
       " u'limited',\n",
       " u'intermeeting',\n",
       " u'unchanged',\n",
       " u'consistent',\n",
       " u'federal',\n",
       " u'vigorous',\n",
       " u'economic',\n",
       " u'likely',\n",
       " u'neutral',\n",
       " u'current',\n",
       " u'unused',\n",
       " u'substantial',\n",
       " u'low',\n",
       " u'accommodative',\n",
       " u'desirable',\n",
       " u'rapid',\n",
       " u'economic',\n",
       " u'press',\n",
       " u'earlier',\n",
       " u'accommodative',\n",
       " u'considerable',\n",
       " u'low',\n",
       " u'current',\n",
       " u'economic',\n",
       " u'desirable',\n",
       " u'near',\n",
       " u'desirable',\n",
       " u'economic',\n",
       " u'specific',\n",
       " u'considerable',\n",
       " u'new',\n",
       " u'important',\n",
       " u'economic',\n",
       " u'unlikely',\n",
       " u'outsized',\n",
       " u'financial',\n",
       " u'sustained',\n",
       " u'financial',\n",
       " u'left',\n",
       " u'little',\n",
       " u'downside',\n",
       " u'economic',\n",
       " u'considerable',\n",
       " u'patient',\n",
       " u'strong',\n",
       " u'special',\n",
       " u'following',\n",
       " u'domestic',\n",
       " u'monetary',\n",
       " u'financial',\n",
       " u'sustainable',\n",
       " u'long-run',\n",
       " u'immediate',\n",
       " u'consistent',\n",
       " u'federal',\n",
       " u'downside',\n",
       " u'sustainable',\n",
       " u'equal',\n",
       " u'unwelcome',\n",
       " u'recent',\n",
       " u'equal',\n",
       " u'low',\n",
       " u'patient',\n",
       " u'economic',\n",
       " u'satisfactory']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []; y = []\n",
    "for key in fomc_mins.keys():\n",
    "    l = []\n",
    "    adjs = fomc_parts[key][1]\n",
    "    for adjlist in adjs:\n",
    "        for a in adjlist:\n",
    "            l.append(a)\n",
    "    X.append(l)\n",
    "    y.append(decisions[key][2])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "itrain, itest = train_test_split(xrange(len(X)), train_size=0.7)\n",
    "mask=np.ones(len(X), dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_xy(X_col, y_col, vectorizer):\n",
    "    X = vectorizer.fit_transform(X_col)\n",
    "    y = y_col\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "log_likelihood\n",
    "\n",
    "Compute the log likelihood of a dataset according to \n",
    "a Naive Bayes classifier. \n",
    "The Log Likelihood is defined by\n",
    "\n",
    "L = Sum_positive(logP(positive)) + Sum_negative(logP(negative))\n",
    "\n",
    "Where Sum_positive indicates a sum over all positive reviews, \n",
    "and Sum_negative indicates a sum over negative reviews\n",
    "    \n",
    "Parameters\n",
    "----------\n",
    "clf : Naive Bayes classifier\n",
    "x : (nexample, nfeature) array\n",
    "    The input data\n",
    "y : (nexample) integer array\n",
    "    Whether each review is Fresh\n",
    "\"\"\"\n",
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten,0].sum() + prob[fresh,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cv_score(clf, x, y, score_func, nfold=5):\n",
    "    \"\"\"\n",
    "    Uses 5-fold cross validation to estimate a score of a classifier\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    clf : Classifier object\n",
    "    x : Input feature vector\n",
    "    y : Input class labels\n",
    "    score_func : Function like log_likelihood, that takes (clf, x, y) as input,\n",
    "                 and returns a score\n",
    "                 \n",
    "    Returns\n",
    "    -------\n",
    "    The average score obtained by splitting (x, y) into 5 folds of training and \n",
    "    test sets, fitting on the training set, and evaluating score_func on the test set\n",
    "    \n",
    "    Examples\n",
    "    cv_score(clf, x, y, log_likelihood)\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf, x[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calibration_plot(clf, xtest, ytest):\n",
    "    prob = clf.predict_proba(xtest)[:, 1]\n",
    "    outcome = ytest\n",
    "    data = pd.DataFrame(dict(prob=prob, outcome=outcome))\n",
    "\n",
    "    #group outcomes into bins of similar probability\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "    cuts = pd.cut(prob, bins)\n",
    "    binwidth = bins[1] - bins[0]\n",
    "    \n",
    "    #freshness ratio and number of examples in each bin\n",
    "    cal = data.groupby(cuts).outcome.agg(['mean', 'count'])\n",
    "    cal['pmid'] = (bins[:-1] + bins[1:]) / 2\n",
    "    cal['sig'] = np.sqrt(cal.pmid * (1 - cal.pmid) / cal['count'])\n",
    "        \n",
    "    #the calibration plot\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    p = plt.errorbar(cal.pmid, cal['mean'], cal['sig'])\n",
    "    plt.plot(cal.pmid, cal.pmid, linestyle='--', lw=1, color='k')\n",
    "    plt.ylabel(\"Empirical P(+)\")\n",
    "    \n",
    "    #the distribution of P(+)\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0), sharex=ax)\n",
    "    \n",
    "    plt.bar(left=cal.pmid - binwidth / 2, height=cal['count'],\n",
    "            width=.95 * (bins[1] - bins[0]),\n",
    "            fc=p[0].get_color())\n",
    "    \n",
    "    plt.xlabel(\"Predicted P(+)\")\n",
    "    plt.ylabel(\"Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-056ba96878c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madjvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mx_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mXtrainthis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mytrainthis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-c1fa6d9da117>\u001b[0m in \u001b[0;36mmake_xy\u001b[1;34m(X_col, y_col, vectorizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_col\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Essam El Messiri\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 804\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Essam El Messiri\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Essam El Messiri\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 236\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Essam El Messiri\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "alphas = [0, .1, 1, 5, 10, 50]\n",
    "min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "maxi = -np.inf; best_alpha = 0; best_min_df = 0\n",
    "for m in min_dfs:\n",
    "    for a in alphas:\n",
    "        vectorizer = CountVectorizer(vocabulary=adjvocab, min_df = m) \n",
    "        x_new, y_new = make_xy(X,y,vectorizer)\n",
    "        Xtrainthis=x_new[mask]\n",
    "        ytrainthis=y_new[mask]\n",
    "        clf = MultinomialNB(alpha=a)\n",
    "        score = cv_score(clf, Xtrainthis, np.ravel(ytrainthis), log_likelihood)\n",
    "        if (score > maxi):\n",
    "            maxi = score\n",
    "            best_alpha = a\n",
    "            best_min_df = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"alpha: %f\" % best_alpha\n",
    "print \"min_df: %f\" % best_min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=adjvocab, min_df=best_min_df)\n",
    "Xnew, ynew = make_xy(X, y, vectorizer)\n",
    "xtrain=Xnew[mask]\n",
    "ytrain=ynew[mask]\n",
    "xtest=Xnew[~mask]\n",
    "ytest=ynew[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print \"Accuracy on training data: %0.4f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.4f\" % (test_accuracy)\n",
    "\n",
    "calibration_plot(clf, xtest, np.ravel(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important features according to our classifier? The visualization belows shows the top 10 features in terms of their log probabilities for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = vectorizer.get_feature_names()\n",
    "feature_probs = clf.feature_log_prob_\n",
    "\n",
    "for a in features_list:\n",
    "    lognegatives[a] = feature_probs[0][i]\n",
    "    logpositives[a] = feature_probs[1][i]\n",
    "    i += 1\n",
    "    \n",
    "ind = logpositives.index(max(logpositives))\n",
    "features_list[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can add bar chart x-axis features (adjectives), y-axis probs*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the visualization. Is it what we thouht it would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Second Approach: Using the Loughran-McDonald Financial Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Adjusting the dictionary for our needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first approach assigned probabilities to words based on the training set of FOMC statements we have. However, the success of this is limited by the number of FOMC statements (182) that we have access to. We can improve this analysis by using more accurate probabilities given by a dictionary that focuses on finance. The **Loughran-McDonald 2014 Master Dictionary** is a great tool for our purposes as it includes words that often appear in 10-K documents and other financial statements. The dictionary includes 9 sentiment categories, including \"negative\", \"positive\", \"uncertainty\", \"litigious\", \"modal\", and \"constraining\", among others. The categories given by this dictionary are more useful to us because of the sheer size of the data set used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data. Below is an example of the data encapsulated in $lmdf$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCUMBER</td>\n",
       "      <td>24454</td>\n",
       "      <td>87222</td>\n",
       "      <td>6.130000e-06</td>\n",
       "      <td>2.570000e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>44863</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENCUMBERED</td>\n",
       "      <td>24455</td>\n",
       "      <td>86996</td>\n",
       "      <td>6.110000e-06</td>\n",
       "      <td>3.480000e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>44409</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENCUMBERING</td>\n",
       "      <td>24456</td>\n",
       "      <td>51445</td>\n",
       "      <td>3.610000e-06</td>\n",
       "      <td>3.660000e-06</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>19907</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENCUMBERS</td>\n",
       "      <td>24457</td>\n",
       "      <td>4835</td>\n",
       "      <td>3.400000e-07</td>\n",
       "      <td>1.380000e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>3258</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENCUMBRANCE</td>\n",
       "      <td>24458</td>\n",
       "      <td>223181</td>\n",
       "      <td>1.570000e-05</td>\n",
       "      <td>6.260000e-06</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>72129</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Sequence Number  Word Count  Word Proportion  \\\n",
       "0     ENCUMBER            24454       87222     6.130000e-06   \n",
       "1   ENCUMBERED            24455       86996     6.110000e-06   \n",
       "2  ENCUMBERING            24456       51445     3.610000e-06   \n",
       "3    ENCUMBERS            24457        4835     3.400000e-07   \n",
       "4  ENCUMBRANCE            24458      223181     1.570000e-05   \n",
       "\n",
       "   Average Proportion   Std Dev  Doc Count  Negative  Positive  Uncertainty  \\\n",
       "0        2.570000e-06  0.000017      44863      2009         0            0   \n",
       "1        3.480000e-06  0.000028      44409      2009         0            0   \n",
       "2        3.660000e-06  0.000050      19907      2009         0            0   \n",
       "3        1.380000e-07  0.000004       3258      2009         0            0   \n",
       "4        6.260000e-06  0.000036      72129      2009         0            0   \n",
       "\n",
       "   Litigious  Constraining  Superfluous  Interesting  Modal  Irr_Verb  \\\n",
       "0       2009          2009            0            0      0         0   \n",
       "1       2009          2009            0            0      0         0   \n",
       "2       2009          2009            0            0      0         0   \n",
       "3       2009          2009            0            0      0         0   \n",
       "4       2009          2009            0            0      0         0   \n",
       "\n",
       "   Harvard_IV  Syllables     Source  \n",
       "0           0          3  12of12inf  \n",
       "1           0          3  12of12inf  \n",
       "2           0          4  12of12inf  \n",
       "3           0          3  12of12inf  \n",
       "4           0          3  12of12inf  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdf = pd.read_csv(\"shortMcDonaldDictionary.csv\")\n",
    "lmdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmdf = lmdf[[\"Word\",\"Negative\",\"Positive\",\"Uncertainty\",\"Litigious\",\"Constraining\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCUMBER</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENCUMBERED</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENCUMBERING</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENCUMBERS</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENCUMBRANCE</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Negative  Positive  Uncertainty  Litigious  Constraining\n",
       "0     ENCUMBER      2009         0            0       2009          2009\n",
       "1   ENCUMBERED      2009         0            0       2009          2009\n",
       "2  ENCUMBERING      2009         0            0       2009          2009\n",
       "3    ENCUMBERS      2009         0            0       2009          2009\n",
       "4  ENCUMBRANCE      2009         0            0       2009          2009"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary includes 9 sentiment categories (e.g. \"negative\", \"positive\", \"uncertainty\", \"litigious\", \"modal\", and \"constraining\"). We reduce the dimensionality of this data down to just three categories: \"negative\", \"positive\", and \"uncertain\". A $word$ that is \"uncertain\" is assigned the probabilities $$P(word_u\\,|\\,-)=P(word_u\\,|\\,+)=\\frac{1}{2}$$\n",
    "We set words in the \"positive\" and \"negative\" parts to have 90% probabilities of being in the group they are assigned to (based on the average probability given by Loughran and McDonald).\n",
    "$$P(word_p\\,|\\,+) = 0.9, P(word_p\\,|\\,-) = 0.1$$\n",
    "$$P(word_n\\,|\\,-) = 0.1, P(word_n\\,|\\,-) = 0.9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjust $lmdf$ to reflect these decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Positive Prob</th>\n",
       "      <th>Negative Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCUMBER</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENCUMBERED</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENCUMBERING</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENCUMBERS</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENCUMBRANCE</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Positive Prob  Negative Prob\n",
       "0     ENCUMBER            0.1            0.9\n",
       "1   ENCUMBERED            0.1            0.9\n",
       "2  ENCUMBERING            0.1            0.9\n",
       "3    ENCUMBERS            0.1            0.9\n",
       "4  ENCUMBRANCE            0.1            0.9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdf_adjusted = lmdf.copy(deep=True)\n",
    "N = 0; P = 0\n",
    "new_values = [0 for i in range(len(lmdf_adjusted.values))]\n",
    "for i, r in enumerate(lmdf_adjusted.values):\n",
    "    if r[1] > 0 or r[4] > 0 or r[5] > 0:\n",
    "        N = 0.9; P = 0.1\n",
    "    elif r[2] > 0:\n",
    "        N = 0.1; P = 0.9\n",
    "    elif r[3] > 0:\n",
    "        N = 0.5; P = 0.5\n",
    "    new_values[i] = [r[0], P, N]\n",
    "\n",
    "final_lmdf = pd.DataFrame(new_values, index=lmdf_adjusted.index, columns = [\"Word\", \"Positive Prob\", \"Negative Prob\"])\n",
    "final_lmdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to calculate the probability of a sentence being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_probs = {}\n",
    "for row in final_lmdf.values:\n",
    "    dict_probs[row[0]] = [row[1], row[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We modify calc_pplus to incorporate our new found knowledge. We now check if a word is in the financial dictionary first.\n",
    "If it is then we use that probabilty, otherwise we use the probability from our earlier Naive Bayes classifier.\n",
    "\n",
    "We also multiply the probabilities from the financial dictionary by a factor to emphasize their importance (based on the\n",
    "fact that we are more confident in their values).\n",
    "\"\"\"\n",
    "\n",
    "def modified_calc_pplus(adjlist, dict_probs, lp, ln, pp, pn):\n",
    "    FACTOR = 1\n",
    "    \n",
    "    prob_p = 0; prob_n = 0\n",
    "    for adj in adjlist:\n",
    "        # Check if the adjective is in the financial dictionary\n",
    "        # If it is take, probability from there\n",
    "        if adj in dict_probs.keys():\n",
    "            prob_p += FACTOR * dict_probs[adj][0]\n",
    "            prob_n += FACTOR * dict_probs[adj][1]\n",
    "        else:    \n",
    "            prob_p += lp[adj]\n",
    "            prob_n += ln[adj]\n",
    "    prob_pos = prob_p * pp\n",
    "    prob_neg = prob_n * pn\n",
    "    prob = float(prob_pos)/(prob_pos + prob_neg)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Most positive and most negative sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential commentary: We can find the most \"positive\" and most \"negative\" sentences and comment on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Classification using new probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: \"Directional Policy\": Lengths of Statements and Federal Reserve Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Regressing outcome against length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Federal Reserve has claimed at several points in the past decade that it attempts to affect financial markets (especially post-crisis) through the release of increasingly long statements that help outline and bring about the changes it hopes for. We take this as a launch pad for another potential statistical analysis that may turn out to be useful for our work. We examine the relationship between the length of statements and their outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the length of statements varies greatly from statement to statement. For example, let us compare two statements from 2014 and 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53905 22899\n"
     ]
    }
   ],
   "source": [
    "print len(pq(fomc_mins['20140730']).text()), len(pq(fomc_mins['20021210']).text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement from 2014 is twice as long as that of 2002 (this is line with the Fed's promise of more locquacious public statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare data to carry out a regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37545</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Y\n",
       "0   62507  0\n",
       "1   56245  0\n",
       "2   24828  0\n",
       "3   47011  0\n",
       "4   37545 -1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "for key in fomc_mins.keys():\n",
    "    new = (len(pq(fomc_mins[key]).text()),decisions[key][2])\n",
    "    length.append(new)\n",
    "    \n",
    "length_data = pd.DataFrame(length, columns = [\"Length\", \"Y\"])\n",
    "length_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm, ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_ols_model = ols('Y ~ Length', length_data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Dec 2015</td> <th>  Prob (F-statistic):</th> <td>0.000246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:01:36</td>     <th>  Log-Likelihood:    </th> <td> -67.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   103</td>      <th>  AIC:               </th> <td>   138.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   101</td>      <th>  BIC:               </th> <td>   143.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.6131</td> <td>    0.148</td> <td>    4.136</td> <td> 0.000</td> <td>    0.319     0.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Length</th>    <td>-1.202e-05</td> <td> 3.16e-06</td> <td>   -3.802</td> <td> 0.000</td> <td>-1.83e-05 -5.75e-06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.995</td> <th>  Durbin-Watson:     </th> <td>   2.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.050</td> <th>  Jarque-Bera (JB):  </th> <td>   5.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.417</td> <th>  Prob(JB):          </th> <td>  0.0556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.807</td> <th>  Cond. No.          </th> <td>1.50e+05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.125\n",
       "Model:                            OLS   Adj. R-squared:                  0.117\n",
       "Method:                 Least Squares   F-statistic:                     14.45\n",
       "Date:                Wed, 09 Dec 2015   Prob (F-statistic):           0.000246\n",
       "Time:                        21:01:36   Log-Likelihood:                -67.120\n",
       "No. Observations:                 103   AIC:                             138.2\n",
       "Df Residuals:                     101   BIC:                             143.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.6131      0.148      4.136      0.000         0.319     0.907\n",
       "Length     -1.202e-05   3.16e-06     -3.802      0.000     -1.83e-05 -5.75e-06\n",
       "==============================================================================\n",
       "Omnibus:                        5.995   Durbin-Watson:                   2.149\n",
       "Prob(Omnibus):                  0.050   Jarque-Bera (JB):                5.780\n",
       "Skew:                          -0.417   Prob(JB):                       0.0556\n",
       "Kurtosis:                       3.807   Cond. No.                     1.50e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.5e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_ols_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Y\n",
       "0   62507  0\n",
       "1   56245  0\n",
       "2   24828  0\n",
       "3   47011  0\n",
       "4   37545  0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in decisions.keys():\n",
    "    if decisions[key][2] == -1:\n",
    "        decisions[key][2] = 0\n",
    "        \n",
    "length = []\n",
    "for key in fomc_mins.keys():\n",
    "    new = (len(pq(fomc_mins[key]).text()),decisions[key][2])\n",
    "    length.append(new)\n",
    "    \n",
    "length_data = pd.DataFrame(length, columns = [\"Length\", \"Y\"])\n",
    "length_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_ols_model = ols('Y ~ Length', length_data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   27.21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Dec 2015</td> <th>  Prob (F-statistic):</th> <td>9.73e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:04:38</td>     <th>  Log-Likelihood:    </th> <td> -31.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   103</td>      <th>  AIC:               </th> <td>   67.59</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   101</td>      <th>  BIC:               </th> <td>   72.86</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.6864</td> <td>    0.105</td> <td>    6.526</td> <td> 0.000</td> <td>    0.478     0.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Length</th>    <td> -1.17e-05</td> <td> 2.24e-06</td> <td>   -5.217</td> <td> 0.000</td> <td>-1.62e-05 -7.25e-06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>17.735</td> <th>  Durbin-Watson:     </th> <td>   2.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  21.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.122</td> <th>  Prob(JB):          </th> <td>1.87e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.197</td> <th>  Cond. No.          </th> <td>1.50e+05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.212\n",
       "Model:                            OLS   Adj. R-squared:                  0.204\n",
       "Method:                 Least Squares   F-statistic:                     27.21\n",
       "Date:                Wed, 09 Dec 2015   Prob (F-statistic):           9.73e-07\n",
       "Time:                        21:04:38   Log-Likelihood:                -31.796\n",
       "No. Observations:                 103   AIC:                             67.59\n",
       "Df Residuals:                     101   BIC:                             72.86\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.6864      0.105      6.526      0.000         0.478     0.895\n",
       "Length      -1.17e-05   2.24e-06     -5.217      0.000     -1.62e-05 -7.25e-06\n",
       "==============================================================================\n",
       "Omnibus:                       17.735   Durbin-Watson:                   2.113\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               21.779\n",
       "Skew:                           1.122   Prob(JB):                     1.87e-05\n",
       "Kurtosis:                       3.197   Cond. No.                     1.50e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.5e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_ols_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comment on the $R^2$ value of our data goes here. General comment about whether or not there is a relationship between length of statement and outcome of Fed statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x29f197b8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Essam El Messiri\\Anaconda\\lib\\site-packages\\matplotlib\\collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAECCAYAAAD6oXArAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVREDq1QuCOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29cc3c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(length_data['Length'], length_data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.304335\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Y</td>        <th>  No. Observations:  </th>  <td>   103</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   101</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 09 Dec 2015</td> <th>  Pseudo R-squ.:     </th>  <td>0.3206</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>21:08:53</td>     <th>  Log-Likelihood:    </th> <td> -31.346</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -46.139</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>5.354e-08</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    3.9017</td> <td>    1.254</td> <td>    3.111</td> <td> 0.002</td> <td>    1.443     6.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Length</th>    <td>   -0.0002</td> <td> 3.89e-05</td> <td>   -3.878</td> <td> 0.000</td> <td>   -0.000 -7.46e-05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   No. Observations:                  103\n",
       "Model:                          Logit   Df Residuals:                      101\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Wed, 09 Dec 2015   Pseudo R-squ.:                  0.3206\n",
       "Time:                        21:08:53   Log-Likelihood:                -31.346\n",
       "converged:                       True   LL-Null:                       -46.139\n",
       "                                        LLR p-value:                 5.354e-08\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      3.9017      1.254      3.111      0.002         1.443     6.360\n",
       "Length        -0.0002   3.89e-05     -3.878      0.000        -0.000 -7.46e-05\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import logit, glm, ols\n",
    "\n",
    "length_logistic = logit('Y ~ Length', length_data).fit()\n",
    "length_logistic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble method that brings it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
