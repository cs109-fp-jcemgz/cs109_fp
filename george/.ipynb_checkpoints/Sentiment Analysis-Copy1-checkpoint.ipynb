{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "We begin by parsing the text and pre-processing it to prepare it for Latent Dirichlet Analysis. This step is meant to remove stopwords and identify nouns and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FOMC statements are full of phrases like \"growth is expected to continue--given the current data--at a moderate pace\". The two hyphens should be treated as a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function to find the nouns and adjectives of the text. The function returns a tuple where the first element is a list of lists, where each list includes the nouns from a sentence. The second element is a list of lists, where each list includes the adjectives from a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modified_get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ', ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        \n",
    "        # Skip the first three sentences that include the HTML\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "#         if i in range(1,4):\n",
    "#             continue\n",
    "            \n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "            \n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2[1:], descriptives2[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the fomc_mins_all dictionary that we created in the Scraping.ipynb iPython Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"fomc_mins_all.json\", \"rb\") as infile:\n",
    "    fomc_mins = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check how our modified_get_parts function would deal with a sample FOMC statement, printing out the first two noun lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'buildmenu',\n",
       "  u'policy',\n",
       "  u'minute',\n",
       "  u'meeting',\n",
       "  u'office',\n",
       "  u'governor',\n",
       "  u'present',\n",
       "  u'member',\n",
       "  u'president',\n",
       "  u'economist',\n",
       "  u'dev'],\n",
       " [u'governor',\n",
       "  u'governor',\n",
       "  u'adviser',\n",
       "  u'governor',\n",
       "  u'governor',\n",
       "  u'governor',\n",
       "  u'merten',\n",
       "  u'interval',\n",
       "  u'meeting',\n",
       "  u'subcommittee',\n",
       "  u'communication',\n",
       "  u'issue']]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_get_parts(pq(fomc_mins['20140730']).text())[0][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run modified_get_parts on each fomc statement and create a new dictionary fomc_parts. We strip each statement of the html at its start before passing it to modified_get_parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fomc_parts = {}\n",
    "for key in fomc_mins.keys():\n",
    "    fomc_parts[key] = modified_get_parts(pq(fomc_mins[key]).text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fomc_parts_file = open(\"fomc_parts.json\", \"wb\")\n",
    "json.dump(fomc_parts, fomc_parts_file)\n",
    "fomc_parts_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create two lists, a list of nouns for each sentence, and a flattened list of all the nouns which we will create to produce a dictionary (needed as an argument for the LDA function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][0]\n",
    "    for nounlist in nouns:\n",
    "        nvocab.append(nounlist)\n",
    "\n",
    "flattenednvocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][0]\n",
    "    for nounlist in nouns:\n",
    "        for n in nounlist:\n",
    "            flattenednvocab.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "frequency = Counter(flattenednvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word = {}; vocab = {}\n",
    "for i,word in enumerate(frequency.keys()):\n",
    "    vocab[word] = i \n",
    "    id2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2759"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def sentencelist(sentence):\n",
    "    d = defaultdict(int); group = []\n",
    "    for word in sentence:\n",
    "        word_id = vocab[word] \n",
    "        d[word_id] += 1 \n",
    "    group = [(a, d[a]) for a in d.keys()]\n",
    "    return group\n",
    "corpus = [sentencelist(sentence) for sentence in nvocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda2 = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = id2word, update_every=1, chunksize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.043*inflation + 0.037*policy + 0.035*market + 0.035*rate + 0.023*member + 0.022*participant + 0.019*meeting + 0.019*period + 0.018*condition + 0.018*fund',\n",
       " u'0.039*price + 0.022*quarter + 0.021*labor + 0.021*increase + 0.019*growth + 0.019*consumer + 0.019*month + 0.017*business + 0.016*spending + 0.016*activity']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Approach: Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Approach: Using the Loughran-McDonald Financial Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first approach assigned probabilities to words based on the training set of FOMC statements we have. However, the success of this is limited by the number of FOMC statements (182) that we have access to. We can improve this analysis by using more accurate probabilities given by a dictionary that focuses on finance. The **Loughran-McDonald 2014 Master Dictionary** is a great tool for our purposes as it includes words that often appear in 10-K documents and other financial statements. The dictionary includes 9 sentiment categories, including \"negative\", \"positive\", \"uncertainty\", \"litigious\", \"modal\", and \"constraining\", among others. The probabilities given by this dictionary are more useful to us because of the sheer size of the data set used to calculate the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the data encapsulated in $lmdict$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary includes 9 sentiment categories (e.g. \"negative\", \"positive\", \"uncertainty\", \"litigious\", \"modal\", and \"constraining\"). We reduce the dimensionality of this data down to just three categories: \"negative\", \"positive\", and \"uncertain\". A $word$ that is \"uncertain\" is assigned the probabilities $$P(word_u\\,|\\,-)=P(word_u\\,|\\,+)=\\frac{1}{2}$$.\n",
    "We set words in the \"positive\" and \"negative\" parts to have 90% probabilities of being in the group they are assigned to (based on the average probability given by Loughran and McDonald).\n",
    "$$P(word_p\\,|\\,+) = 0.9, P(word_p\\,|\\,-) = 0.1$$\n",
    "$$P(word_n\\,|\\,-) = 0.1, P(word_n\\,|\\,-) = 0.9$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
