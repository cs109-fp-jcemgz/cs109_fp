{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Sentiment Analysis on FOMC Statements](#Sentiment-Analysis)\n",
    "\t* [Part 1: Preparations and Topic Analysis](#Q1-Predicting-Elections-from-Samples)\n",
    "\t\t* [1.1 Natural Language Processing](#Natural-Language-Processing)\n",
    "\t\t* [1.2 Topic Extraction](#Topic-Extraction)\n",
    "    * [Part 2: Naive Bayes Classification](#Part-2:-Naive-Bayes-Approach)\n",
    "        * [2.1 First Approach: Training a Naive Bayes Classifier](#2.1:-First-Approach:-Training-a-Naive-Bayes-Classifier)\n",
    "        * [2.2 Finding the most important features](#2.2:-Most-important-features)\n",
    "    * [Part 3: Second Approach: Using the Loughran-McDonald Financial Dictionary](#Part-3:-Second-Approach:-Using-the-Loughran-McDonald-Financial-Dictionary)\n",
    "        * [3.1: Adjusting the dictionary for our needs](#3.1:-Adjusting-the-dictionary-for-our-needs)\n",
    "        * [3.2: Most positive and most negative sentences](#3.2:-Most-positive-and-most-negative-sentences)\n",
    "        * [3.3: Classification using new probabilities](#3.3:-Classification-using-new-probabilities)\n",
    "    * [Part 4: \"Directional Policy\": Lengths of Statements and Federal Reserve Policy](#Part-4:-\"Directional-Policy\":-Lengths-of-Statements-and-Federal-Reserve-Policy)\n",
    "        * [4.1: Regressing outcome against length](#4.1:-Regressing-outcome-against-length)\n",
    "        * [4.2: Commentary](#4.2: Commentary)\n",
    "    * [Part 5: Ensemble method](#Part-5:-Ensemble-method)\n",
    "\t\t\t\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pyquery import PyQuery as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preparations and Topic Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "We begin by parsing the text and pre-processing it to prepare it for Latent Dirichlet Analysis. This step is meant to remove stopwords and identify nouns and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FOMC statements are full of phrases like \"growth is expected to continue--given the current data--at a moderate pace\". The two hyphens should be treated as a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex1=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function to find the nouns and adjectives of the text. The function returns a tuple where the first element is a list of lists, where each list includes the nouns from a sentence. The second element is a list of lists, where each list includes the adjectives from a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modified_get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ', ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        \n",
    "        # Skip the first three sentences that include the HTML\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "#         if i in range(1,4):\n",
    "#             continue\n",
    "            \n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "            \n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2[1:], descriptives2[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the fomc_mins_all dictionary that we created in the Scraping.ipynb iPython Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"fomc_mins_all.json\", \"rb\") as infile:\n",
    "    fomc_mins = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check how our modified_get_parts function would deal with a sample FOMC statement, printing out the first two noun lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'buildmenu',\n",
       "  u'policy',\n",
       "  u'minute',\n",
       "  u'meeting',\n",
       "  u'office',\n",
       "  u'governor',\n",
       "  u'present',\n",
       "  u'member',\n",
       "  u'president',\n",
       "  u'economist',\n",
       "  u'dev'],\n",
       " [u'governor',\n",
       "  u'governor',\n",
       "  u'adviser',\n",
       "  u'governor',\n",
       "  u'governor',\n",
       "  u'governor',\n",
       "  u'merten',\n",
       "  u'interval',\n",
       "  u'meeting',\n",
       "  u'subcommittee',\n",
       "  u'communication',\n",
       "  u'issue']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_get_parts(pq(fomc_mins['20140730']).text())[0][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run modified_get_parts on each fomc statement and create a new dictionary fomc_parts. We strip each statement of the html at its start before passing it to modified_get_parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fomc_parts = {}\n",
    "for key in fomc_mins.keys():\n",
    "    fomc_parts[key] = modified_get_parts(pq(fomc_mins[key]).text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fomc_parts_file = open(\"fomc_parts.json\", \"wb\")\n",
    "json.dump(fomc_parts, fomc_parts_file)\n",
    "fomc_parts_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create two lists, a list of nouns for each sentence, and a flattened list of all the nouns which we will create to produce a dictionary (needed as an argument for the LDA function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][0]\n",
    "    for nounlist in nouns:\n",
    "        nvocab.append(nounlist)\n",
    "\n",
    "flattenednvocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][0]\n",
    "    for nounlist in nouns:\n",
    "        for n in nounlist:\n",
    "            flattenednvocab.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "frequency = Counter(flattenednvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word = {}; vocab = {}\n",
    "for i,word in enumerate(frequency.keys()):\n",
    "    vocab[word] = i \n",
    "    id2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2759"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def sentencelist(sentence):\n",
    "    d = defaultdict(int); group = []\n",
    "    for word in sentence:\n",
    "        word_id = vocab[word] \n",
    "        d[word_id] += 1 \n",
    "    group = [(a, d[a]) for a in d.keys()]\n",
    "    return group\n",
    "corpus = [sentencelist(sentence) for sentence in nvocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda2 = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = id2word, update_every=1, chunksize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.041*inflation + 0.040*price + 0.036*rate + 0.029*growth + 0.017*increase + 0.017*labor + 0.016*year + 0.016*quarter + 0.016*policy + 0.015*fund',\n",
       " u'0.041*market + 0.025*policy + 0.023*activity + 0.020*security + 0.016*period + 0.016*meeting + 0.014*member + 0.014*sale + 0.012*participant + 0.012*condition']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some commment on the topic extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2552, 1)]\n",
      "[(0, 0.37866949748678369), (1, 0.62133050251321631)]\n",
      "member\n",
      "==========================================\n",
      "[(2120, 1), (1600, 1), (2356, 1), (2463, 1)]\n",
      "[(0, 0.1001005985760791), (1, 0.89989940142392089)]\n",
      "dealer issue bid ability\n",
      "==========================================\n",
      "[(1493, 1), (133, 1), (2575, 1)]\n",
      "[(0, 0.12503062011603921), (1, 0.87496937988396073)]\n",
      "paragraph currency transaction\n",
      "==========================================\n",
      "[(162, 1), (139, 1), (2134, 1), (1038, 1), (830, 1)]\n",
      "[(0, 0.085582647537895948), (1, 0.91441735246210409)]\n",
      "purchase end exchange sale spot\n",
      "==========================================\n",
      "[(2584, 1), (521, 1), (13, 1), (1590, 1)]\n",
      "[(0, 0.10005311135878529), (1, 0.89994688864121475)]\n",
      "drawing character swap operation\n",
      "==========================================\n",
      "[(672, 2), (19, 1), (387, 1), (104, 1), (1771, 1), (1712, 1), (67, 1), (1398, 1), (2552, 1)]\n",
      "[(0, 0.098435568814343982), (1, 0.9015644311856561)]\n",
      "time risk approach assessment announcement wording balance condition member\n",
      "==========================================\n",
      "[(775, 1), (841, 1), (903, 1), (2255, 1), (913, 1), (2643, 1), (1225, 1), (2617, 1), (922, 1), (890, 1)]\n",
      "[(0, 0.79675803902220066), (1, 0.20324196097779931)]\n",
      "market recovery quarter labor year employment improvement loss increase half\n",
      "==========================================\n",
      "[(2047, 1)]\n",
      "[(0, 0.74994327378130465), (1, 0.25005672621869529)]\n",
      "income\n",
      "==========================================\n",
      "[(922, 1), (1475, 1), (1060, 1), (613, 1), (1038, 1)]\n",
      "[(0, 0.28589204710742344), (1, 0.71410795289257656)]\n",
      "increase book-value ratio lows sale\n",
      "==========================================\n",
      "[(876, 1), (942, 1), (1135, 1), (2357, 1), (1240, 1), (1083, 1)]\n",
      "[(0, 0.91128723065204276), (1, 0.088712769347957326)]\n",
      "policy accommodation period slack inflation resource\n",
      "==========================================\n",
      "[(1520, 1), (630, 1), (2563, 1), (2500, 1), (2117, 2), (775, 1), (1672, 1), (1997, 2), (2637, 1), (2255, 1), (1424, 1), (1253, 1), (131, 1), (2548, 1), (2371, 1), (1398, 2), (920, 1), (2047, 1)]\n",
      "[(0, 0.66604076742699392), (1, 0.33395923257300614)]\n",
      "response strengthening effect addition business market financing spending growth labor investment output household productivity acceleration condition profit income\n",
      "==========================================\n",
      "[(1632, 1), (736, 1), (2117, 2), (1289, 1), (1869, 1), (1997, 1), (2680, 1), (820, 1), (1941, 1), (886, 1), (1943, 1), (2552, 1), (1019, 1), (476, 1), (2174, 1)]\n",
      "[(0, 0.55308858850216913), (1, 0.44691141149783087)]\n",
      "number sign business industry development spending software discussion indication nation softness member equipment inventory confidence\n",
      "==========================================\n",
      "[(129, 1), (2563, 1), (1030, 1), (777, 1), (876, 1), (2189, 1), (334, 1), (1997, 1), (1297, 1), (2174, 1)]\n",
      "[(0, 0.80788410701897706), (1, 0.19211589298102297)]\n",
      "estate effect price wealth policy equity consumer spending outlook confidence\n",
      "==========================================\n",
      "[(2000, 1), (2442, 1), (1002, 1)]\n",
      "[(0, 0.85690070618054959), (1, 0.14309929381945052)]\n",
      "disinflation view prospect\n",
      "==========================================\n",
      "[(1632, 1), (1604, 1), (997, 1), (775, 2), (876, 1), (942, 1), (1712, 1), (1074, 1), (19, 1), (116, 1), (759, 1), (2552, 1), (2713, 1)]\n",
      "[(0, 0.4637606448845043), (1, 0.53623935511549581)]\n",
      "number circumstance future market policy accommodation wording expectation risk room change member valuation\n",
      "==========================================\n",
      "[(963, 1), (2575, 1)]\n",
      "[(0, 0.16671241077186569), (1, 0.83328758922813428)]\n",
      "vote transaction\n",
      "==========================================\n",
      "[(1418, 1), (2637, 1), (1135, 1), (22, 1), (567, 1), (921, 1), (187, 1), (383, 1)]\n",
      "[(0, 0.94011375661535568), (1, 0.059886243384644419)]\n",
      "rate growth period rise manufacturing factory production capacity\n",
      "==========================================\n",
      "[(1036, 1), (1268, 1), (1540, 1), (1564, 1), (263, 1)]\n",
      "[(0, 0.91664214925776821), (1, 0.083357850742231857)]\n",
      "average deficit trade good service\n",
      "==========================================\n",
      "[(1826, 1), (775, 1), (1418, 1), (1135, 1), (2738, 1), (1590, 1)]\n",
      "[(0, 0.5929161974507472), (1, 0.40708380254925275)]\n",
      "fund market rate period percent operation\n",
      "==========================================\n",
      "[(2563, 1), (2223, 1), (2117, 1), (1030, 1), (1831, 1), (2344, 1), (777, 1), (1997, 1), (2695, 1), (2189, 1), (334, 1), (2637, 2), (720, 1), (88, 1), (922, 1), (1019, 1), (2170, 1)]\n",
      "[(0, 0.89595050273005306), (1, 0.10404949726994694)]\n",
      "effect stock business price demand durable wealth spending housing equity consumer growth item unit increase equipment buildup\n",
      "==========================================\n",
      "[(1632, 1), (736, 1), (1831, 2), (1225, 1), (398, 1), (310, 1), (567, 1), (2552, 1), (409, 1)]\n",
      "[(0, 0.20550078966211152), (1, 0.7944992103378884)]\n",
      "number sign demand improvement extent strength manufacturing member country\n",
      "==========================================\n",
      "[(1312, 1), (966, 1), (1382, 1), (2701, 1), (1745, 1), (2552, 1), (1657, 1)]\n",
      "[(0, 0.067426336676460197), (1, 0.93257366332353986)]\n",
      "area city apartment building activity member oversupply\n",
      "==========================================\n",
      "[(1665, 1), (1794, 1), (1731, 1), (2500, 1), (2245, 1), (1030, 1), (1583, 1), (1074, 1), (2563, 1), (1374, 1), (1302, 1), (2359, 1), (1240, 1), (922, 1), (126, 1)]\n",
      "[(0, 0.84967052970230039), (1, 0.15032947029769955)]\n",
      "appreciation supply factor addition reversal price energy expectation effect dollar dissipation compensation inflation increase example\n",
      "==========================================\n",
      "[(2118, 1), (2506, 1), (1259, 1), (876, 1), (1135, 1), (2552, 1), (957, 1), (2079, 1)]\n",
      "[(0, 0.38596313373227042), (1, 0.61403686626772958)]\n",
      "directive proposal respect policy period member symmetry adjustment\n",
      "==========================================\n",
      "[(1665, 1), (133, 2), (870, 1), (775, 1), (2287, 1), (1135, 1), (511, 1), (2134, 1), (2543, 1), (1721, 1), (2523, 1), (1374, 1), (223, 1)]\n",
      "[(0, 0.037974957037168681), (1, 0.96202504296283142)]\n",
      "appreciation currency group market subset period bit exchange partner relation value dollar trading\n",
      "==========================================\n",
      "[(963, 1), (2575, 1)]\n",
      "[(0, 0.16671241072473589), (1, 0.83328758927526414)]\n",
      "vote transaction\n",
      "==========================================\n",
      "[(2640, 1), (88, 1), (476, 1), (717, 1), (1135, 1)]\n",
      "[(0, 0.40824277041595763), (1, 0.59175722958404242)]\n",
      "home unit inventory level period\n",
      "==========================================\n",
      "[(1240, 1), (1583, 1), (1030, 1), (478, 1), (1839, 1)]\n",
      "[(0, 0.91192597670962527), (1, 0.088074023290374781)]\n",
      "inflation energy price rebound pace\n",
      "==========================================\n",
      "[(133, 1), (870, 1), (775, 1), (1135, 1), (2134, 1), (2543, 1), (1721, 1), (2523, 1), (1374, 1), (223, 1)]\n",
      "[(0, 0.048780713779763958), (1, 0.9512192862202361)]\n",
      "currency group market period exchange partner relation value dollar trading\n",
      "==========================================\n",
      "[(1585, 1), (826, 1), (485, 1), (2638, 1), (1831, 1)]\n",
      "[(0, 0.76649497271160105), (1, 0.233505027288399)]\n",
      "slowdown economy hand export demand\n",
      "==========================================\n",
      "[(162, 1), (837, 1), (1997, 1), (946, 1), (22, 1), (759, 1), (1849, 1)]\n",
      "[(0, 0.49016174351939712), (1, 0.50983825648060299)]\n",
      "purchase date spending advance rise change century\n",
      "==========================================\n",
      "[(2552, 1), (1240, 1), (922, 1), (1030, 1), (937, 1)]\n",
      "[(0, 0.90021979866078305), (1, 0.099780201339217015)]\n",
      "member inflation increase price possibility\n",
      "==========================================\n",
      "[(2240, 1), (1505, 1), (1135, 1), (1869, 1), (1325, 1), (1840, 1), (1297, 1), (2552, 1), (633, 1), (826, 2), (411, 1), (1240, 1), (1023, 1)]\n",
      "[(0, 0.41155156320472952), (1, 0.58844843679527037)]\n",
      "light information period development regard behavior outlook member evidence economy uncertainty inflation performance\n",
      "==========================================\n",
      "[(2708, 1), (717, 1)]\n",
      "[(0, 0.3660835488186403), (1, 0.63391645118135964)]\n",
      "construction level\n",
      "==========================================\n",
      "[(1826, 1), (997, 1), (1030, 1), (775, 1), (1418, 1), (1036, 1), (2637, 1), (337, 1), (2738, 1), (1022, 1), (1398, 1), (862, 1)]\n",
      "[(0, 0.9546568787915255), (1, 0.04534312120847455)]\n",
      "fund future price market rate average growth reserve percent objective condition stability\n",
      "==========================================\n",
      "[(896, 1), (997, 1), (1398, 1), (1590, 2), (1239, 1)]\n",
      "[(0, 0.095069489746940114), (1, 0.90493051025305982)]\n",
      "term future condition operation test\n",
      "==========================================\n",
      "[(672, 1), (1607, 1), (1418, 1), (2738, 1), (1498, 1), (444, 1), (346, 1)]\n",
      "[(0, 0.93228220054425914), (1, 0.067717799455740885)]\n",
      "time reason rate percent share worker unemployment\n",
      "==========================================\n",
      "[(1776, 1), (2128, 1), (50, 1), (922, 1), (1564, 1), (1599, 1)]\n",
      "[(0, 0.92392940546462776), (1, 0.076070594535372252)]\n",
      "aircraft capital net increase good shipment\n",
      "==========================================\n",
      "[(1312, 1), (2178, 1), (2211, 1), (903, 1), (2637, 1), (1520, 1), (922, 1), (478, 1), (1727, 1)]\n",
      "[(0, 0.66841424096768531), (1, 0.33158575903231474)]\n",
      "area consumption tax quarter growth response increase rebound indicator\n",
      "==========================================\n",
      "[(1632, 1), (2272, 1), (1731, 1), (775, 1), (2698, 1), (839, 1), (1038, 1), (368, 1), (1810, 1), (1178, 1), (1435, 1), (954, 1), (1418, 1), (223, 1)]\n",
      "[(0, 0.44425478266371193), (1, 0.55574521733628801)]\n",
      "number investor factor market trigger data sale release strategy participant movement swing rate trading\n",
      "==========================================\n",
      "[(2272, 1), (1795, 1), (101, 1), (2566, 1), (775, 1), (2388, 1)]\n",
      "[(0, 0.072975756017954835), (1, 0.92702424398204508)]\n",
      "investor loan asset issuance market class\n",
      "==========================================\n",
      "[(166, 1), (775, 1), (1385, 1), (523, 1), (1294, 2), (1103, 1), (409, 1)]\n",
      "[(0, 0.058572275986921149), (1, 0.94142772401307895)]\n",
      "greek market program spread bond concern country\n",
      "==========================================\n",
      "[(19, 1), (257, 1), (2415, 1), (104, 1), (876, 2), (2637, 1), (1869, 1), (1297, 1), (1107, 1), (2103, 1), (1240, 1), (826, 1), (1103, 1)]\n",
      "[(0, 0.50005580122166304), (1, 0.49994419877833701)]\n",
      "risk downside staff assessment policy growth development outlook forecast shock inflation economy concern\n",
      "==========================================\n",
      "[(1762, 1), (2637, 1), (1745, 1), (2708, 1), (1974, 1), (1178, 1)]\n",
      "[(0, 0.34258273999571198), (1, 0.65741726000428802)]\n",
      "project growth activity construction pipeline participant\n",
      "==========================================\n",
      "[(896, 1), (1188, 1), (1030, 2), (844, 1), (1804, 1), (1110, 1), (1240, 1), (1178, 1)]\n",
      "[(0, 0.9443195233063113), (1, 0.055680476693688773)]\n",
      "term oil price commodity decline import inflation participant\n",
      "==========================================\n",
      "[(672, 1), (1826, 1), (101, 1), (1385, 1), (1418, 1), (139, 1), (162, 1), (1978, 1), (1652, 1), (948, 1), (1178, 1), (282, 1)]\n",
      "[(0, 0.41321330940596229), (1, 0.58678669059403776)]\n",
      "time fund asset program rate end purchase range statement language participant target\n",
      "==========================================\n",
      "[(485, 1), (488, 1), (2637, 1), (1869, 1), (373, 1), (1398, 1)]\n",
      "[(0, 0.70008975320350841), (1, 0.29991024679649164)]\n",
      "hand opportunity growth development reference condition\n",
      "==========================================\n",
      "[(1398, 1), (1590, 1), (775, 1)]\n",
      "[(0, 0.14446913010488613), (1, 0.85553086989511395)]\n",
      "condition operation market\n",
      "==========================================\n",
      "[(862, 1), (1030, 1), (2643, 1), (2462, 1), (310, 1), (826, 1), (1790, 1)]\n",
      "[(0, 0.90705092164874301), (1, 0.092949078351256889)]\n",
      "stability price employment progress strength economy context\n",
      "==========================================\n",
      "[(1600, 1), (809, 1), (1387, 1), (876, 1), (2415, 1), (2257, 1), (1178, 1), (1978, 1)]\n",
      "[(0, 0.69327999883818869), (1, 0.30672000116181131)]\n",
      "issue topic presentation policy staff communication participant range\n",
      "==========================================\n",
      "[(2242, 1), (549, 1), (1000, 1), (1516, 1), (175, 1), (1135, 1), (2552, 2), (1817, 1), (1755, 1)]\n",
      "[(0, 0.049510305240145526), (1, 0.95048969475985456)]\n",
      "agenda election oath advice office period member meeting individual\n",
      "==========================================\n",
      "[(963, 1), (1062, 1)]\n",
      "[(0, 0.16710783017775618), (1, 0.8328921698222439)]\n",
      "vote title\n",
      "==========================================\n",
      "[(2273, 1), (939, 1), (1101, 1), (206, 1), (2611, 1), (53, 1), (542, 2)]\n",
      "[(0, 0.055567268632810557), (1, 0.94443273136718942)]\n",
      "day repurchase calendar bank government agreement security\n",
      "==========================================\n",
      "[(1674, 1), (963, 1), (1062, 1), (1259, 1)]\n",
      "[(0, 0.10016265210761584), (1, 0.89983734789238423)]\n",
      "instruction vote title respect\n",
      "==========================================\n",
      "[(1817, 1), (714, 1), (963, 1), (1458, 1)]\n",
      "[(0, 0.10272122036204427), (1, 0.89727877963795566)]\n",
      "meeting minute vote action\n",
      "==========================================\n",
      "[(2212, 1), (717, 1), (2381, 1), (913, 1), (1375, 1), (567, 1), (187, 1), (444, 1), (1087, 2)]\n",
      "[(0, 0.94453013082385973), (1, 0.055469869176140267)]\n",
      "high level post-world year month manufacturing production worker workweek\n",
      "==========================================\n",
      "[(896, 1), (903, 1), (76, 1), (2060, 1), (2637, 1), (1424, 1)]\n",
      "[(0, 0.35022352181025174), (1, 0.64977647818974826)]\n",
      "term quarter permit structure growth investment\n",
      "==========================================\n",
      "[(2116, 1), (1030, 2), (717, 1), (334, 1), (1583, 1), (922, 1)]\n",
      "[(0, 0.93662193435444785), (1, 0.063378065645552165)]\n",
      "index price level consumer energy increase\n",
      "==========================================\n",
      "[(133, 1), (1745, 1), (681, 1), (529, 1), (1143, 1), (1374, 1)]\n",
      "[(0, 0.077774002899267572), (1, 0.92222599710073239)]\n",
      "currency activity datum background mark dollar\n",
      "==========================================\n",
      "[(717, 1), (1831, 1), (1997, 1), (1839, 1), (1721, 1), (922, 1), (863, 1)]\n",
      "[(0, 0.52548491035727518), (1, 0.47451508964272476)]\n",
      "level demand spending pace relation increase course\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "for bow in corpus[0:900:15]:\n",
    "    print bow\n",
    "    print lda2.get_document_topics(bow)\n",
    "    print \" \".join([id2word[e[0]] for e in bow])\n",
    "    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment on carrying out naive bayes and the fact that we will do it 2 ways.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: First Approach: Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the vocabulary of adjectives that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flattened_adj_vocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][1]\n",
    "    for nounlist in nouns:\n",
    "        for n in nounlist:\n",
    "            flattened_adj_vocab.append(n)\n",
    "\n",
    "frequency2 = Counter(flattened_adj_vocab)\n",
    "id2adj = {}; adjvocab = {}\n",
    "for i,word in enumerate(frequency2.keys()):\n",
    "    adjvocab[word] = i \n",
    "    id2adj[i] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the adjectives in each statement into a single list and thereby have a list of adjectives for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adjvocab = []\n",
    "for key in fomc_mins.keys():\n",
    "    nouns = fomc_parts[key][1]\n",
    "    for nounlist in nouns:\n",
    "        adjvocab.append(nounlist)\n",
    "X = adjvocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Response Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"actions_05-15.json\", \"rb\") as infile:\n",
    "    decisions = json.load(infile)\n",
    "len(decisions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions_work = decisions.copy()\n",
    "new_decisions = {}\n",
    "for key in fomc_mins:\n",
    "    if key in decisions_work.keys():\n",
    "        new_decisions[key] = decisions_work[key][-1]\n",
    "        del decisions_work[key]\n",
    "        \n",
    "len(new_decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a group of statements and decisions, where the statements are releases made at certain dates and decisions are interest rate decisions made at certain dates. We now group the statements between each decision into one string. For example, if a decision is made on 09/28/2014 and the next one is made on 10/18/2014, then the statements in between would be grouped together. This allows us to match the dimensions of our input and our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "itrain, itest = train_test_split(xrange(len(X)), train_size=0.7)\n",
    "mask=np.ones(len(X), dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_xy(X_col, y_col, vectorizer):\n",
    "    X = vectorizer.fit_transform(X_col)\n",
    "    y = y_col\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "log_likelihood\n",
    "\n",
    "Compute the log likelihood of a dataset according to \n",
    "a Naive Bayes classifier. \n",
    "The Log Likelihood is defined by\n",
    "\n",
    "L = Sum_positive(logP(positive)) + Sum_negative(logP(negative))\n",
    "\n",
    "Where Sum_positive indicates a sum over all positive reviews, \n",
    "and Sum_negative indicates a sum over negative reviews\n",
    "    \n",
    "Parameters\n",
    "----------\n",
    "clf : Naive Bayes classifier\n",
    "x : (nexample, nfeature) array\n",
    "    The input data\n",
    "y : (nexample) integer array\n",
    "    Whether each review is Fresh\n",
    "\"\"\"\n",
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten,0].sum() + prob[fresh,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cv_score(clf, x, y, score_func, nfold=5):\n",
    "    \"\"\"\n",
    "    Uses 5-fold cross validation to estimate a score of a classifier\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    clf : Classifier object\n",
    "    x : Input feature vector\n",
    "    y : Input class labels\n",
    "    score_func : Function like log_likelihood, that takes (clf, x, y) as input,\n",
    "                 and returns a score\n",
    "                 \n",
    "    Returns\n",
    "    -------\n",
    "    The average score obtained by splitting (x, y) into 5 folds of training and \n",
    "    test sets, fitting on the training set, and evaluating score_func on the test set\n",
    "    \n",
    "    Examples\n",
    "    cv_score(clf, x, y, log_likelihood)\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf, x[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calibration_plot(clf, xtest, ytest):\n",
    "    prob = clf.predict_proba(xtest)[:, 1]\n",
    "    outcome = ytest\n",
    "    data = pd.DataFrame(dict(prob=prob, outcome=outcome))\n",
    "\n",
    "    #group outcomes into bins of similar probability\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "    cuts = pd.cut(prob, bins)\n",
    "    binwidth = bins[1] - bins[0]\n",
    "    \n",
    "    #freshness ratio and number of examples in each bin\n",
    "    cal = data.groupby(cuts).outcome.agg(['mean', 'count'])\n",
    "    cal['pmid'] = (bins[:-1] + bins[1:]) / 2\n",
    "    cal['sig'] = np.sqrt(cal.pmid * (1 - cal.pmid) / cal['count'])\n",
    "        \n",
    "    #the calibration plot\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    p = plt.errorbar(cal.pmid, cal['mean'], cal['sig'])\n",
    "    plt.plot(cal.pmid, cal.pmid, linestyle='--', lw=1, color='k')\n",
    "    plt.ylabel(\"Empirical P(+)\")\n",
    "    \n",
    "    #the distribution of P(+)\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0), sharex=ax)\n",
    "    \n",
    "    plt.bar(left=cal.pmid - binwidth / 2, height=cal['count'],\n",
    "            width=.95 * (bins[1] - bins[0]),\n",
    "            fc=p[0].get_color())\n",
    "    \n",
    "    plt.xlabel(\"Predicted P(+)\")\n",
    "    plt.ylabel(\"Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"alpha: %f\" % best_alpha\n",
    "print \"min_df: %f\" % best_min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=adjvocab, min_df=best_min_df)\n",
    "Xnew, ynew = make_xy(X, y, vectorizer)\n",
    "xtrain=Xnew[mask]\n",
    "ytrain=ynew[mask]\n",
    "xtest=Xnew[~mask]\n",
    "ytest=ynew[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print \"Accuracy on training data: %0.4f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.4f\" % (test_accuracy)\n",
    "\n",
    "calibration_plot(clf, xtest, np.ravel(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important features according to our classifier? The visualization belows shows the top 10 features in terms of their log probabilities for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can add bar chart x-axis features (adjectives), y-axis probs*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the visualization. Is it what we thouht it would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Second Approach: Using the Loughran-McDonald Financial Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Adjusting the dictionary for our needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first approach assigned probabilities to words based on the training set of FOMC statements we have. However, the success of this is limited by the number of FOMC statements (182) that we have access to. We can improve this analysis by using more accurate probabilities given by a dictionary that focuses on finance. The **Loughran-McDonald 2014 Master Dictionary** is a great tool for our purposes as it includes words that often appear in 10-K documents and other financial statements. The dictionary includes 9 sentiment categories, including \"negative\", \"positive\", \"uncertainty\", \"litigious\", \"modal\", and \"constraining\", among others. The categories given by this dictionary are more useful to us because of the sheer size of the data set used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data. Below is an example of the data encapsulated in $lmdf$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCUMBER</td>\n",
       "      <td>24454</td>\n",
       "      <td>87222</td>\n",
       "      <td>6.130000e-06</td>\n",
       "      <td>2.570000e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>44863</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENCUMBERED</td>\n",
       "      <td>24455</td>\n",
       "      <td>86996</td>\n",
       "      <td>6.110000e-06</td>\n",
       "      <td>3.480000e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>44409</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENCUMBERING</td>\n",
       "      <td>24456</td>\n",
       "      <td>51445</td>\n",
       "      <td>3.610000e-06</td>\n",
       "      <td>3.660000e-06</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>19907</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENCUMBERS</td>\n",
       "      <td>24457</td>\n",
       "      <td>4835</td>\n",
       "      <td>3.400000e-07</td>\n",
       "      <td>1.380000e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>3258</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENCUMBRANCE</td>\n",
       "      <td>24458</td>\n",
       "      <td>223181</td>\n",
       "      <td>1.570000e-05</td>\n",
       "      <td>6.260000e-06</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>72129</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Sequence Number  Word Count  Word Proportion  \\\n",
       "0     ENCUMBER            24454       87222     6.130000e-06   \n",
       "1   ENCUMBERED            24455       86996     6.110000e-06   \n",
       "2  ENCUMBERING            24456       51445     3.610000e-06   \n",
       "3    ENCUMBERS            24457        4835     3.400000e-07   \n",
       "4  ENCUMBRANCE            24458      223181     1.570000e-05   \n",
       "\n",
       "   Average Proportion   Std Dev  Doc Count  Negative  Positive  Uncertainty  \\\n",
       "0        2.570000e-06  0.000017      44863      2009         0            0   \n",
       "1        3.480000e-06  0.000028      44409      2009         0            0   \n",
       "2        3.660000e-06  0.000050      19907      2009         0            0   \n",
       "3        1.380000e-07  0.000004       3258      2009         0            0   \n",
       "4        6.260000e-06  0.000036      72129      2009         0            0   \n",
       "\n",
       "   Litigious  Constraining  Superfluous  Interesting  Modal  Irr_Verb  \\\n",
       "0       2009          2009            0            0      0         0   \n",
       "1       2009          2009            0            0      0         0   \n",
       "2       2009          2009            0            0      0         0   \n",
       "3       2009          2009            0            0      0         0   \n",
       "4       2009          2009            0            0      0         0   \n",
       "\n",
       "   Harvard_IV  Syllables     Source  \n",
       "0           0          3  12of12inf  \n",
       "1           0          3  12of12inf  \n",
       "2           0          4  12of12inf  \n",
       "3           0          3  12of12inf  \n",
       "4           0          3  12of12inf  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdf = pd.read_csv(\"shortMcDonaldDictionary.csv\")\n",
    "lmdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmdf = lmdf[[\"Word\",\"Negative\",\"Positive\",\"Uncertainty\",\"Litigious\",\"Constraining\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCUMBER</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENCUMBERED</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENCUMBERING</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENCUMBERS</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENCUMBRANCE</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Negative  Positive  Uncertainty  Litigious  Constraining\n",
       "0     ENCUMBER      2009         0            0       2009          2009\n",
       "1   ENCUMBERED      2009         0            0       2009          2009\n",
       "2  ENCUMBERING      2009         0            0       2009          2009\n",
       "3    ENCUMBERS      2009         0            0       2009          2009\n",
       "4  ENCUMBRANCE      2009         0            0       2009          2009"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary includes 9 sentiment categories (e.g. \"negative\", \"positive\", \"uncertainty\", \"litigious\", \"modal\", and \"constraining\"). We reduce the dimensionality of this data down to just three categories: \"negative\", \"positive\", and \"uncertain\". A $word$ that is \"uncertain\" is assigned the probabilities $$P(word_u\\,|\\,-)=P(word_u\\,|\\,+)=\\frac{1}{2}$$\n",
    "We set words in the \"positive\" and \"negative\" parts to have 90% probabilities of being in the group they are assigned to (based on the average probability given by Loughran and McDonald).\n",
    "$$P(word_p\\,|\\,+) = 0.9, P(word_p\\,|\\,-) = 0.1$$\n",
    "$$P(word_n\\,|\\,-) = 0.1, P(word_n\\,|\\,-) = 0.9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjust $lmdf$ to reflect these decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Positive Prob</th>\n",
       "      <th>Negative Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCUMBER</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENCUMBERED</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENCUMBERING</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENCUMBERS</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENCUMBRANCE</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Positive Prob  Negative Prob\n",
       "0     ENCUMBER            0.1            0.9\n",
       "1   ENCUMBERED            0.1            0.9\n",
       "2  ENCUMBERING            0.1            0.9\n",
       "3    ENCUMBERS            0.1            0.9\n",
       "4  ENCUMBRANCE            0.1            0.9"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdf_adjusted = lmdf.copy(deep=True)\n",
    "N = 0; P = 0\n",
    "new_values = [0 for i in range(len(lmdf_adjusted.values))]\n",
    "for i, r in enumerate(lmdf_adjusted.values):\n",
    "    if r[1] > 0 or r[4] > 0 or r[5] > 0:\n",
    "        N = 0.9; P = 0.1\n",
    "    elif r[2] > 0:\n",
    "        N = 0.1; P = 0.9\n",
    "    elif r[3] > 0:\n",
    "        N = 0.5; P = 0.5\n",
    "    new_values[i] = [r[0], P, N]\n",
    "\n",
    "final_lmdf = pd.DataFrame(new_values, index=lmdf_adjusted.index, columns = [\"Word\", \"Positive Prob\", \"Negative Prob\"])\n",
    "final_lmdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to calculate the probability of a sentence being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_probs = {}\n",
    "for row in final_lmdf.values:\n",
    "    dict_probs[row[0]] = [row[1], row[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We modify calc_pplus to incorporate our new found knowledge. We now check if a word is in the financial dictionary first.\n",
    "If it is then we use that probabilty, otherwise we use the probability from our earlier Naive Bayes classifier.\n",
    "\n",
    "We also multiply the probabilities from the financial dictionary by a factor to emphasize their importance (based on the\n",
    "fact that we are more confident in their values).\n",
    "\"\"\"\n",
    "\n",
    "def modified_calc_pplus(adjlist, dict_probs, lp, ln, pp, pn):\n",
    "    FACTOR = 1\n",
    "    \n",
    "    prob_p = 0; prob_n = 0\n",
    "    for adj in adjlist:\n",
    "        # Check if the adjective is in the financial dictionary\n",
    "        # If it is take, probability from there\n",
    "        if adj in dict_probs.keys():\n",
    "            prob_p += FACTOR * dict_probs[adj][0]\n",
    "            prob_n += FACTOR * dict_probs[adj][1]\n",
    "        else:    \n",
    "            prob_p += lp[adj]\n",
    "            prob_n += ln[adj]\n",
    "    prob_pos = prob_p * pp\n",
    "    prob_neg = prob_n * pn\n",
    "    prob = float(prob_pos)/(prob_pos + prob_neg)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Most positive and most negative sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential commentary: We can find the most \"positive\" and most \"negative\" sentences and comment on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Classification using new probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: \"Directional Policy\": Lengths of Statements and Federal Reserve Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Regressing outcome against length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Federal Reserve has claimed at several points in the past decade that it attempts to affect financial markets (especially post-crisis) through the release of increasingly long statements that help outline and bring about the changes it hopes for. We take this as a launch pad for another potential statistical analysis that may turn out to be useful for our work. We examine the relationship between the length of statements and their outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm, ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_ols_model = ols('Y ~ Length', length_data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length_ols_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comment on the $R^2$ value of our data goes here. General comment about whether or not there is a relationship between length of statement and outcome of Fed statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble method that brings it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
